<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.15">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dan Simpson">
<meta name="dcterms.date" content="2021-12-09">
<meta name="description" content="A repost from Andrew’s blog about how design information infects multivariate priors. (Lightly edited. Well, a bit more than lightly because the last version didn’t fully make sense. But whatever. Blogs, eh.) Original posted 5 November, 2017.">

<title>Un garçon pas comme les autres (Bayes) - Why won’t you cheat with me? (Repost)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Un garçon pas comme les autres (Bayes) - Why won’t you cheat with me? (Repost)">
<meta property="og:description" content="A repost from Andrew’s blog about how design information infects multivariate priors. (Lightly edited. Well, a bit more than lightly because the last version didn’t fully make sense. But whatever. Blogs, eh.) Original posted 5 November, 2017.">
<meta property="og:image" content="https://dansblog.netlify.app/posts/2021-12-09-why-wont-you-cheat-with-me-repost/sylvia2.JPG">
<meta property="og:site-name" content="Un garçon pas comme les autres (Bayes)">
<meta name="twitter:title" content="Why won’t you cheat with me? (Repost)">
<meta name="twitter:description" content="A repost from Andrew’s blog about how design information infects multivariate priors. (Lightly edited. Well, a bit more than lightly because the last version didn’t fully make sense. But whatever. Blogs, eh.) Original posted 5 November, 2017.">
<meta name="twitter:image" content="https://dansblog.netlify.app/posts/2021-12-09-why-wont-you-cheat-with-me-repost/sylvia2.JPG">
<meta name="twitter:creator" content="@dan_p_simpson">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Un garçon pas comme les autres (Bayes)</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About this blog</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/dan_p_simpson"><i class="bi bi-twitter" role="img" aria-label="twitter">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/dpsimpson"><i class="bi bi-github" role="img" aria-label="github">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://dansblog.netlify.app"><i class="bi bi-person-circle" role="img" aria-label="website">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Why won’t you cheat with me? (Repost)</h1>
                  <div>
        <div class="description">
          <p>A repost from Andrew’s blog about how design information infects multivariate priors. (Lightly edited. Well, a bit more than lightly because the last version didn’t fully make sense. But whatever. Blogs, eh.) Original posted 5 November, 2017.</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Prior distributions</div>
                <div class="quarto-category">Fundamentals</div>
                <div class="quarto-category">Design dependence</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Dan Simpson </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 9, 2021</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sparsity" id="toc-sparsity" class="nav-link active" data-scroll-target="#sparsity">Sparsity</a>
  <ul class="collapse">
  <li><a href="#the-design-scaling-in-these-priors-links-directly-to-an-implied-decision-process" id="toc-the-design-scaling-in-these-priors-links-directly-to-an-implied-decision-process" class="nav-link" data-scroll-target="#the-design-scaling-in-these-priors-links-directly-to-an-implied-decision-process">The design-scaling in these priors links directly to an implied decision process</a></li>
  </ul></li>
  <li><a href="#models-specified-through-their-full-conditionals" id="toc-models-specified-through-their-full-conditionals" class="nav-link" data-scroll-target="#models-specified-through-their-full-conditionals">Models specified through their full conditionals</a>
  <ul class="collapse">
  <li><a href="#design-dependence-for-markov-random-fields" id="toc-design-dependence-for-markov-random-fields" class="nav-link" data-scroll-target="#design-dependence-for-markov-random-fields">Design dependence for Markov random fields</a></li>
  </ul></li>
  <li><a href="#gaussian-process-models" id="toc-gaussian-process-models" class="nav-link" data-scroll-target="#gaussian-process-models">Gaussian process models</a>
  <ul class="collapse">
  <li><a href="#design-for-gaussian-processes-id-say-designing-women-but-im-aware-of-the-demographics" id="toc-design-for-gaussian-processes-id-say-designing-women-but-im-aware-of-the-demographics" class="nav-link" data-scroll-target="#design-for-gaussian-processes-id-say-designing-women-but-im-aware-of-the-demographics">Design for Gaussian processes (I’d say “Designing Women”, but I’m aware of the demographics)</a></li>
  </ul></li>
  <li><a href="#principles-can-only-get-you-so-far" id="toc-principles-can-only-get-you-so-far" class="nav-link" data-scroll-target="#principles-can-only-get-you-so-far">Principles can only get you so far</a></li>
  </ul>
</nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<blockquote class="blockquote">
<p>But I got some ground rules&nbsp; I’ve found to be sound rules<br>
and you’re not the one I’m exempting.<br>
Nonetheless, I confess it’s tempting.<br>
– <a href="https://www.youtube.com/watch?v=K2sPdIsr7jY">Jenny Toomey sings Franklin Bruno</a></p>
</blockquote>
<p>It turns out that I did something a little controversial in <a href="https://dansblog.netlify.app/posts/2021-12-08-the-king-must-die-repost/">last week’s</a><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> post. As these things always go, it wasn’t the thing I was expecting to get push back from, but rather what I thought was a fairly innocuous scaling of the prior. <a href="http://statmodeling.stat.columbia.edu/2017/11/02/king-must-die/#comment-601142">One commenter</a> (and a few other people on other communication channels) pointed out that the dependence of the prior on the design didn’t seem kosher. Of course, we (Andrew, Mike and I) wrote a paper that was sort of about this a <a href="http://www.stat.columbia.edu/~gelman/research/published/entropy-19-00555-v2.pdf">few months ago</a><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, but it’s one of those really interesting topics that we can probably all deal with thinking more about.</p>
<p>So in this post, I’m going to go into a couple of situations where it makes sense to scale the prior based on fixed information about the experiment. (The emerging theme for these posts is “things I think are interesting and useful but are probably not publishable” interspersed with “weird digressions into musical theatre / the personal mythology of Patti LuPone”.)</p>
<p>If you haven’t clicked yet, this particular post is going to be drier than Eve Arden in Mildred Pierce. If you’d rather be entertained, I’d recommend <a href="https://open.spotify.com/album/2qY9GSG0nLoJdcQNmYxMGE">Tempting: Jenny Toomey sings the songs of Franklin Bruno</a>. (Franklin Bruno is today’s stand in for Patti, because I’m still sad that War Paint closed<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. I only got to see it twice.)</p>
<p>(Jenny Toomey was one of the most exciting American indie musicians in the 90s both through her bands [Tsunami was the notable one, but there were others] and her work with Simple Machines, the label she co-founded. These days she’s working in musician advocacy and hasn’t released an album since the early 2000s. Bruno’s current band is called The Human Hearts. He has had a long solo career and was also in an excellent powerpop band called Nothing Painted Blue, who had an album called The Monte Carlo Method. And, now<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> that I live in Canada, I should say that that album has a fabulous cover of Mark Szabo’s I Should Be With You. To be honest, the only reason I work with Andrew and the Stan crew is that I figure if I’m in New York often enough I’ll eventually coincide with a Human Hearts concert<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.)</p>
<section id="sparsity" class="level2">
<h2 class="anchored" data-anchor-id="sparsity">Sparsity</h2>
<blockquote class="blockquote">
<p>Why won’t you cheat with me? You and I both know you’ve done it before. – <a href="https://www.youtube.com/watch?v=dL-4ZQthJ5w">Jenny Toomey sings Franklin Bruno</a></p>
</blockquote>
<p>The first object of our affliction are priors that promote sparsity in high-dimensional models. There has been a lot of work on this topic, but the cheaters guide is basically this:</p>
<blockquote class="blockquote">
<p>While spike-and-slab models can exactly represent sparsity and have excellent theoretical properties, they are basically useless from a computational point of view. So we use scale-mixture of normal priors (also known as local-global priors) to achieve approximate sparsity, and then use some sort of decision rule to take our approximately sparse signal and make it exactly sparse.</p>
</blockquote>
<p>What is a scale-mixture of normals? Well it has the general form <span class="math display">\[
\beta_j \sim N(0, \tau^2 \psi^2_j),
\]</span> where <span class="math inline">\(\tau\)</span> is a global standard deviation parameter, controlling how large the <span class="math inline">\(\beta_j\)</span> parameters are in general<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>, while the local standard deviation parameters <span class="math inline">\(\psi_j\)</span> control how big <span class="math inline">\(\beta_j\)</span> is <em>relative</em> to the other <span class="math inline">\(\beta\)</span>s.</p>
<p>The priors for <span class="math inline">\(\tau\)</span> and the <span class="math inline">\(\psi_j\)</span> are typically set to be independent. A lot of theoretical work just treats <span class="math inline">\(\tau\)</span> as fixed (or as otherwise less important than the local parameters), but <a href="https://arxiv.org/abs/1610.05559">this is wrong</a>.</p>
<p><em>Pedant’s corner:</em> Andrew likes define mathematical statisticians as those who use <span class="math inline">\(x\)</span> for their data rather than <span class="math inline">\(y\)</span>. I prefer to characterise them by those who think it’s a good idea to put a prior on variance (an un-elicitable quantity) rather than standard deviation (which is easy to have opinions about). Please people just stop doing this. You’re not helping yourselves!</p>
<p>Actually, maybe that last point isn’t for Pedant’s Corner after all. Because if you parameterise by standard deviation it’s pretty easy to work out what the marginal prior on <span class="math inline">\(\beta_j\)</span> (with <span class="math inline">\(\tau\)</span> fixed) is.</p>
<p>This is quite useful because, with the notable exception of the “Bayesian” “Lasso” <a href="https://dansblog.netlify.app/posts/2021-12-08-the-king-must-die-repost/">which-does-not-work-but-will-never-die-because-it-was-inexplicably-published-in-the leading-stats-journal-by-prominent-statisticians-and-has-the-word-Lasso-in-the-title-even-though-a-back-of-the-envelope-calculation-or-I-don’t-know-a-fairly-straightforward-simulation-by-the-reviewers-should-have-nixed-it</a> (to use its married name), we can’t compute the marginal prior for most scale-mixtures of normals.</p>
<p>The following result, which was killed by reviewers at some point during the PC prior papers long review process, but lives forever <a href="https://arxiv.org/abs/1403.4630v1">in the arXiv’d first version</a>, tells you everything you need to know. It’s a picture because frankly I’ve had a glass of wine and I’m not bloody typing it all again<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>.</p>
<div id="thm-prior" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 </strong></span>Let <span class="math inline">\(\pi_d(r)\)</span> be a prior on the standard deviation of <span class="math inline">\(v \sim {\mathcal N}(0,r^2)\)</span>. The induced prior <span class="math display">\[
\pi(v) = \int_0^\infty
\frac{1}{2\pi r}\exp\left({-\frac{v^2}{2r^2}}\right)\pi_d(r)\,dr
\]</span> has the following properties. Fix <span class="math inline">\(\delta&gt; 0\)</span>.</p>
<ol type="1">
<li><p>If <span class="math inline">\(\pi_d(r) \leq Cr^t\)</span> for all <span class="math inline">\(r \in [0,\delta]\)</span> and for some <span class="math inline">\(C,t &gt;0\)</span>, then <span class="math inline">\(\pi(v)\)</span> is finite at <span class="math inline">\(v=0\)</span>.</p></li>
<li><p>If <span class="math inline">\(\pi_d(r) \in (0,\infty)\)</span> for every <span class="math inline">\(r \in [0,\delta]\)</span>, then <span class="math inline">\(\pi(v)\)</span> has a weak logarithmic spike at zero, that is <span class="math display">\[
\pi(v) = \mathcal{O}\left[\log\left(1 +
                                   \frac{\delta^2}{v^2}\right)\right], \qquad v \rightarrow 0.
\]</span></p></li>
<li><p>If <span class="math inline">\(\int_0^\delta \frac{1}{2\pi  r}\exp\left({-\frac{v^2}{2r^2}}\right)\pi_d(r)\,dr &lt;  \infty\)</span>, then <span class="math display">\[
\pi(v) \geq
\mathcal{O}\left(v^{-2}\exp\left(-\frac{v^2}{2\delta^2}\right)\right),
\qquad |v| \rightarrow \infty.
\]</span></p></li>
<li><p>If <span class="math inline">\(\pi_d(r) {\leq}({\geq}) Cr^{-t}\)</span> for all <span class="math inline">\(r \in [0,\delta]\)</span> and for some <span class="math inline">\(C,t &gt;0\)</span>, then <span class="math display">\[
\pi(v)
{\leq}({\geq}) \mathcal{O}(|v|^{-t}),\qquad v \rightarrow 0.
\]</span></p></li>
<li><p>If <span class="math inline">\(\pi_d(r) {\leq}({\geq}) Cr^{-t}\)</span> for all <span class="math inline">\(r &gt;\delta\)</span> and for some <span class="math inline">\(C,t &gt;0\)</span>, then <span class="math display">\[
\pi(v)
{\leq}({\geq}) \mathcal{O}(|v|^{-t}),\qquad |v| \rightarrow
\infty.
\]</span></p></li>
</ol>
</div>
<details>
<summary>
The proof is here.
</summary>
<p>For any <span class="math inline">\(\delta &gt; 0\)</span>, <span class="math display">\[
\pi(v) =
\int_0^\delta\frac{1}{2\pi r}
\exp\left({-\frac{v^2}{2r^2}}\right)
\pi_d(r)\,dr +
\int_\delta^\infty\frac{1}{2\pi
r}\exp\left({-\frac{v^2}{2r^2}}\right)
\pi_d(r)\,dr = I_1 + I_2.
\]</span> Examining this splitting, we note that <span class="math inline">\(I_1\)</span> will control the behaviour of <span class="math inline">\(\pi(v)\)</span> near zero, while <span class="math inline">\(I_2\)</span> will control the tails.</p>
<p>Assuming that <span class="math inline">\(\int_\delta^\infty r^{-1}\pi_d(r)\,dr &lt; \infty\)</span>, we can bound <span class="math inline">\(I_2\)</span> as <span class="math display">\[
\frac{1}{2\pi }\exp\left({-\frac{v^2}{2\delta^2}}\right)
\int_\delta^\infty r^{-1}\pi_d(r)\,dr \leq I_2 \leq \frac{1}{2\pi}
\int_\delta^\infty r^{-1}\pi_d(r)\,dr.
\]</span></p>
<p>To prove part 1, let <span class="math inline">\(\pi_d(r) \leq Cr^t\)</span>, <span class="math inline">\(r \in [0,\delta]\)</span> for some <span class="math inline">\(t&gt;0\)</span>. Substituting this into <span class="math inline">\(I_1\)</span> and computing the resulting integral using Maple<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>, we get <span class="math display">\[\begin{align*}
I_1 &amp;\leq - \frac{C}{2\pi t}\left( {2}^{-1/2\,t}{|v|}^{t}\Gamma
\left( 1-1/2\,t,1/2\,{\frac {v^2}{{\delta}^{2}}} \right) -{{\rm
e}^{-1/2\,{\frac {v^2}{{\delta}^{2}}}} }{\delta}^{t}
\right) = \mathcal{O}(1),
\end{align*}\]</span> where <span class="math inline">\(\Gamma(a,x) = \int_x^\infty \exp\left({-t}\right)t^{a-1}\,dt\)</span> is the incomplete Gamma function.</p>
<p>To prove parts 2 and 3, we bound <span class="math inline">\(I_1\)</span> as follows. <span class="math display">\[\begin{align*}
\left(\inf_{r\in[0,\delta]} \pi_d(r)
\right)\int_0^\delta\frac{1}{2\pi
r}\exp\left({-\frac{v^2}{2r^2}}\right) \,dr &amp;\leq I_1 \leq
\left(\sup_{r\in[0,\delta]} \pi_d(r)
\right)\int_0^\delta\frac{1}{2\pi r}\exp\left({-\frac{v^2}{2r^2}}\right) \\
\frac{1}{4\pi}\left(\inf_{r\in[0,\delta]} \pi_d(r)\right)
\text{E}_1\left(\frac{v^2}{2\delta^2}\right) &amp; \leq I_1 \leq
\frac{1}{4\pi}\left(\sup_{r\in[0,\delta]} \pi_d(r)\right)
\text{E}_1\left(\frac{v^2}{2\delta^2}\right) \\
\frac{1}{8\pi}\left(\inf_{r\in[0,\delta]} \pi_d(r)\right)
\exp\left({-\frac{v^2}{2\delta^2}}\right)\log\left( 1 +
\frac{4\delta^2}{v^2}\right) &amp;\leq I_1
\leq\frac{1}{4\pi}\left(\sup_{r\in[0,\delta]} \pi_d(r)\right)
\exp\left({-\frac{v^2}{2\delta^2}}\right)\log\left( 1 +
\frac{2\delta^2}{v^2}\right),
\end{align*}\]</span> where <span class="math inline">\(\text{E}_1(x) = \int_1^\infty t^{-1}\exp\left({-tx}\right)\,dt\)</span> and the third line of inequalities follows using standard bounds in the exponential integral<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>.</p>
<p>Combining the lower and upper bounds, it follows that if <span class="math inline">\(0 &lt;\inf_{r\in[0,\delta]} \pi_d(r) \leq \sup_{r\in[0,\delta]} \pi_d(r) &lt; \infty\)</span>, then <span class="math inline">\(\pi(v)\)</span> has a logarithmic spike near zero. Similarly, the lower bounds show that <span class="math inline">\(\pi(v) \geq C v^{-2}\exp\left(-\frac{v^2}{2\delta^2}\right)\)</span> as <span class="math inline">\(v\rightarrow \infty\)</span>.</p>
<p>Part 4 follows by considering let <span class="math inline">\(\pi_d(r) = Cr^{-t}\)</span>, <span class="math inline">\(r \in [0,\delta]\)</span> for some <span class="math inline">\(t&gt;0\)</span>. Substituting this into <span class="math inline">\(I_1\)</span> and computing the resulting integral using Maple, we get <span class="math display">\[\begin{align*}
I_1 &amp; = \frac{C}{2\pi t}\left( {|v|}^{-t}\Gamma \left(
1+1/2\,t,1/2\,{\frac {v^2}{{\delta}^{2}}} \right)
{2}^{t/2}-{\delta}^{-t}{{\rm e}^{-1/2\,{\frac
{v^2}{{\delta}^{2}}}}} \right) \sim
\mathcal{O}(v^{-t})
\end{align*}\]</span> as <span class="math inline">\(v \rightarrow 0\)</span>. We note that <span class="math inline">\(I_1 = \mathcal{O}\left(\exp\left(-v^2/(2\delta^2)\right)\right)\)</span> as <span class="math inline">\(|v| \rightarrow \infty\)</span>.</p>
<p>To prove part 5, let <span class="math inline">\(\pi_d(r) = Cr^{-t}\)</span>, <span class="math inline">\(r \in (\delta,\infty)\)</span> for some <span class="math inline">\(t&gt;0\)</span>. Substituting this into <span class="math inline">\(I_2\)</span>, we get <span class="math display">\[\begin{align*}
I_2 = \frac{C}{8\pi^2}\,{2}^{1/2\,t}{|v|}^{-t} \left( \Gamma
\left( 1/2\,t \right) - \Gamma \left( 1/2\,t,1/2\,{\frac
{{v}^{2}}{{\delta}^{2}}} \right) \right) =
\mathcal{O}(|v|^{-t}),
\end{align*}\]</span> where we used the identity <span class="math display">\[
\Gamma \left( 1/2\,t \right) - \Gamma
\left( 1/2\,t,1/2\,{\frac {{v}^{2}}{{\delta}^{2}}} \right)
\rightarrow \Gamma\left( 1/2\,t \right)
\]</span> as <span class="math inline">\(|v|\rightarrow \infty\)</span>.</p>
<strong>Done.</strong>
</details>
<p>All of this basically says the following:</p>
<ul>
<li><p>If the density of the prior on the standard deviation is finite at zero, then the implied prior on <span class="math inline">\(\beta_j\)</span> has a logarithmic spike at zero.</p></li>
<li><p>If the density of the prior on the standard has a polynomial tail, then the implied prior on <span class="math inline">\(\beta_j\)</span> has the same polynomial tail.</p></li>
<li><p>Not in the result, but computed at the time: if the prior on the standard deviation is exponential, the prior on <span class="math inline">\(\beta_j\)</span> still has Gaussian-ish tails. I couldn’t work out what happened in the hinterland between exponential tails and polynomial tails, but I suspect at some point the tail on the standard deviation does eventually get heavy enough to be seen in the marginal, but I can’t tell you when.)</p></li>
</ul>
<p>With this sort of information, you can compute the equivalent of the bounds that I did on the Laplace prior for the general case (or, actually, for the case that will have at least a little bit of a chance, which is the monotonically decreasing priors on the standard deviation).</p>
<p>In particular, <a href="https://dansblog.netlify.app/posts/2021-12-08-the-king-must-die-repost/">if you run the argument from the last post</a>, you see that you need a quite heavy tail on the standard deviation prior to get a reasonable prior on the implied sparsity. In particular, <a href="https://arxiv.org/pdf/1403.4630v4.pdf">we showed</a> that applying this reasoning to the horseshoe prior, where the prior on the local standard deviation is half-Cauchy, you can see that there is a <span class="math inline">\(\lambda\)</span> that gives <em>a priori</em> weight on <span class="math inline">\(p^{-1}\)</span>-sparse signals, while also letting you have a few very large <span class="math inline">\(\beta_j\)</span>s.</p>
<section id="the-design-scaling-in-these-priors-links-directly-to-an-implied-decision-process" class="level3">
<h3 class="anchored" data-anchor-id="the-design-scaling-in-these-priors-links-directly-to-an-implied-decision-process">The design-scaling in these priors links directly to an implied decision process</h3>
<blockquote class="blockquote">
<p>You’d look better if your shadow didn’t follow you around, but it looks as though you’re tethered to the ground, just like every pound of flesh I’ve ever found. – <a href="https://www.youtube.com/watch?v=mIp4X7_cA3g">Franklin Bruno in a sourceless light</a>.</p>
</blockquote>
<p>For a very simple decision process (the deterministic threshold process described in the previous post), you can work out exactly how the threshold needs to interact with the prior. In particular, we can see that if we’re trying to detect a true signal that is exactly zero (no components are active), then we know that <span class="math inline">\(latex \| \mathbf{X} \boldsymbol{\beta} \| = 0\)</span>. This is not possible for these scale-mixture models, but we can require that in this case all of the components are at most <span class="math inline">\(latex \epsilon\)</span>, in which case <span class="math display">\[
\| \mathbf{X}\boldsymbol{\beta} \| \leq \epsilon \| \mathbf{X} \|,
\]</span> which suggests we want <span class="math inline">\(\epsilon \ll \| \mathbf{X} \|_\infty^{-1}\)</span>. The calculation in the previous post shows that if we want this sort of almost zero signal to have any mass at all under the prior, we need to scale <span class="math inline">\(\lambda\)</span> using information about <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>Of course, this is a very very simple decision process. I have absolutely no idea how to repeat these arguments for actually good decision processes, like the predictive loss minimization <a href="https://arxiv.org/abs/1707.01694">favoured by Aki</a>. But I’d still expect that we’d need to make sure there was a priori enough mass in the areas of the parameter space where the decision process is firmly one way or another (as well as <a href="https://statmodeling.stat.columbia.edu/2017/10/29/contour-as-a-verb/">mass in the indeterminate region</a>). I doubt that the Bayesian Lasso would magically start to work under these more complex losses.</p>
</section>
</section>
<section id="models-specified-through-their-full-conditionals" class="level2">
<h2 class="anchored" data-anchor-id="models-specified-through-their-full-conditionals">Models specified through their full conditionals</h2>
<blockquote class="blockquote">
<p>Why won’t you cheat with me? You and I both know that he’s done the same. – <a href="https://www.youtube.com/watch?v=Ozsc2AQqYKw">Franklin Bruno</a></p>
</blockquote>
<p>So we can view the design dependence of sparsity priors as preparation for the forthcoming decision process. (Those of you who just mentally broke into <a href="https://www.youtube.com/watch?v=c1SiaCV26aQ">Prepare Ye The Way Of The Lord</a> from Godspell, please come to the front of the class. You are my people.) Now let’s talk about a case where this isn’t true.</p>
<p>To do this, we need to cast our minds back to a time when people really did have the original cast recording of Godspell on their mind. In particular, we need to think about <a href="https://www.youtube.com/watch?v=pqoeM18vCaU">Julian Besag</a> (who I’m sure was really into musicals about Jesus. I have no information to the contrary, so I’m just going to assume it’s true.) who wrote a series of important papers, one in <a href="https://www.jstor.org/stable/2984812">1974</a> and one in <a href="https://www.jstor.org/stable/2987782">1975</a> (and several before and after, but I can’t be arsed linking to them all. We all have google.) about specifying models through conditional independence relations.</p>
<p>These models have a special place in time series modelling (where we all know about discrete-time Markovian processes) and in spatial statistics. In particular, generalisations of Besag’s (Gaussian) conditional autoregressive (CAR) models are w<a href="https://arxiv.org/abs/1601.01180">idely used in spatial epidemiology</a>.</p>
<p>Mathematically, Gaussian CAR models (and more generally <a href="https://www.routledge.com/Gaussian-Markov-Random-Fields-Theory-and-Applications/Rue-Held/p/book/9781584884323">Gaussian Markov random fields</a> on graphs) are defined through their precision matrix, that is the inverse of the covariance matrix as <span class="math display">\[
\mathbf{x} \sim N(\mathbf{0}, \tau^{-1}\mathbf{Q}^{-1}).
\]</span></p>
<p>For simple models, such as the popular CAR model, we assume <span class="math inline">\(\mathbf{Q}\)</span> is fixed, known, and sparse (i.e.&nbsp;it has a lot of zeros) and we typically interpret <span class="math inline">\(\tau\)</span> to be the inverse of the variance of <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>This interpretation of <span class="math inline">\(\tau\)</span> could not be more wrong.</p>
<p>Why? Well, let’s look at the marginal distribution <span class="math display">\[
x_j \sim N\left(0, \tau^{-1}[Q^{-1}]_{ii}\right).
\]</span></p>
<p>To interpet <span class="math inline">\(\tau\)</span> and the inverse variance, we need the diagonal elements of <span class="math inline">\(\mathbf{Q}^{-1}\)</span> to all be around 1. <em>This is never the case.</em></p>
<p>A simple, mathematically tractable example is the first order random walk on a one-dimensional lattice, which can be written in terms of the increment process as <span class="math display">\[
x_{j+1} - x_j \sim N(0, \tau^{-1}), \qquad j = 1, \ldots J-1.
\]</span></p>
<p>Conditioned on a particular starting point, this process looks a lot like a discrete version of Brownian motion as you move the lattice points closer together. This is a useful model for rough non-linear random effects, such as the baseline hazard rate in a Cox proportional hazard model. A long and detailed (and quite general) discussion of these models can be found in <a href="https://www.routledge.com/Gaussian-Markov-Random-Fields-Theory-and-Applications/Rue-Held/p/book/9781584884323">Rue and Held’s book</a>.</p>
<p>I am bringing this case up because you can actually work out the size of the diagonal of <span class="math inline">\(\mathbf{Q}^{-1}\)</span>. <a href="https://www.sciencedirect.com/science/article/abs/pii/S2211675313000407">Sørbye and Rue</a> talk about this in detail, but for this model maybe the easiest way to understand it is that if we had a fixed lattice with <span class="math inline">\(n\)</span> points and we’d carefully worked out a sensible prior for <span class="math inline">\(\tau\)</span>. Now imagine that we’ve gotten some new data and instead of only <span class="math inline">\(n\)</span> points in the lattice, we got information at a finer scale, so now the same interval is covered by <span class="math inline">\(nk\)</span> equally spaced nodes. We model this with the new first order random walk prior <span class="math display">\[
x'_{j+1} - x'_j \sim N(0,[\tau']^{-1}).
\]</span></p>
<p>It turns out that we can relate the inverse variances of these two increment processes as <span class="math inline">\(\tau' = J \tau\)</span>.</p>
<p>This strongly suggests that we should not use the same prior for <span class="math inline">\(\tau\)</span> as we should for <span class="math inline">\(\tau'\)</span>, but that the prior should actually know about how many nodes there are on the lattice. Concrete suggestions are in the Sørbye and Rue paper linked above.</p>
<section id="design-dependence-for-markov-random-fields" class="level3">
<h3 class="anchored" data-anchor-id="design-dependence-for-markov-random-fields">Design dependence for Markov random fields</h3>
<blockquote class="blockquote">
<p>Not to coin a phrase, but play it as it lays – <a href="https://open.spotify.com/track/5gs7YbjVEjjKMNNO219iJe?si=e20a167de87a4dc4">Franklin Bruno in Nothing Painted Blue</a></p>
</blockquote>
<p>This type of design dependence is a general problem for multivariate Gaussian models specified through their precision (so-called Gaussian Markov random fields). The critical thing here is that, unlike the sparsity case, the design dependence does not come from some type of decision process. It comes from the gap between the parameterisation (in terms of <span class="math inline">\(\tau\)</span> and <span class="math inline">\(\mathbf{Q}\)</span>) and the elicitable quantity (the scale of the random effect).</p>
<p>This is kinda a general lesson. <em>When specifying multivariate priors, you must always check the implications of your prior on the one- and two-dimensional quantities of interest. Because weird things happen in multivariate land!</em></p>
</section>
</section>
<section id="gaussian-process-models" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-process-models">Gaussian process models</h2>
<blockquote class="blockquote">
<p>And it’s not like we’re tearing down a house of more than gingerbread. It’s not like we’re calling down the wrath of heaven on our heads. – <a href="(https://www.youtube.com/watch?v=dL-4ZQthJ5w)">Jenny Toomey sings Franklin Bruno</a></p>
</blockquote>
<p>So the design dependence doesn’t necessarily come in preparation for some kind of decision, it can also be because we have constructed (and therefore parameterised) our process in an inconvenient way. Let’s see if we can knock out another one before my bottle of wine dies.</p>
<p><a href="https://dansblog.netlify.app/posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/">Gaussian processes</a>, the least exciting tool in the machine learner’s toolbox, are another example where your priors need to be design dependent. It will probably surprise you not a single sausage that in this case the need for design dependence comes from a completely different place.</p>
<p>For simplicity let’s consider a Gaussian process <span class="math inline">\(f(t)\)</span> in one dimension with isotropic covariance function <span class="math display">\[
c(s,t) =\sigma^2 (\kappa|s-t|)^\nu K_\nu(|\kappa|s-t|).
\]</span></p>
<p>This is the commonly encountered Whittle-Matérn family of covariance functions. The distinguished members are the exponential covariance function when <span class="math inline">\(\nu = 0.5\)</span> and the squared exponential function <span class="math display">\[
c(s,t)= \sigma^2\exp\left(\kappa |s-t|^2 \right),
\]</span></p>
<p>which is the limit as <span class="math inline">\(\nu \rightarrow \infty\)</span>.</p>
<p>One of the inconvenient features of Matérn models in 1-3 dimensions is that it is impossible to consistently recover all of the parameters by simply observing more and more of the random effect on a fixed interval. You need to see new replicates in order to properly pin these down<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>.</p>
<p>So one might expect that this non-identifiability would be the source of some problems.</p>
<p>One would be wrong.</p>
<p>The squared exponential covariance function does not have this pathology, but it’s still very very hard to fit. Why? Well the problem is that you can interpret <span class="math inline">\(\kappa\)</span> as an inverse-range parameter. Roughly, the interpretation is that if <span class="math display">\[
|s - t | &gt; \frac{ \sqrt{ 8 \nu } }{\kappa}
\]</span> then the value of <span class="math inline">\(u(s)\)</span> is approximately independent of the value of <span class="math inline">\(u(t)\)</span>.</p>
<p>This means that a fixed data set provides no information about <span class="math inline">\(\kappa\)</span> in large parts of the parameter space. In particular if <span class="math inline">\(\kappa^{-1}\)</span> is bigger than the range of the measurement locations, then the data has almost no information about the parameter.</p>
<p>Similarly, if <span class="math inline">\(\kappa^{-1}\)</span> is smaller than the smallest distance between two data points (or for irregular data, this should be something like “smaller than some low quantile of the set of distances between points”), then the data will have nothing to say about the parameter.</p>
<p>Of these two scenarios, it turns out that the inference is much less sensitive to the prior on small values of <span class="math inline">\(\kappa\)</span> (ie ranges longer than the data) than it is on small values of <span class="math inline">\(\kappa\)</span> (ie ranges shorter than the data).</p>
<p>Currently, we have two recommendations: one based around <a href="https://arxiv.org/abs/1503.00256">PC priors</a> and a very similar one based around <a href="https://mc-stan.org/docs/2_28/stan-users-guide/fit-gp.html#priors-gp.section">inverse gamma priors</a>. But both of these require you to specify the design-dependent quantity of a “minimum length scale we expect this data set to be informative about”.</p>
<section id="design-for-gaussian-processes-id-say-designing-women-but-im-aware-of-the-demographics" class="level3">
<h3 class="anchored" data-anchor-id="design-for-gaussian-processes-id-say-designing-women-but-im-aware-of-the-demographics">Design for Gaussian processes (I’d say “Designing Women”, but I’m aware of the demographics)</h3>
<blockquote class="blockquote">
<p>I’m a disaster, you’re a disaster, we’re a disaster area. – Franklin Bruno in The Human Hearts (featuring alto extraordinaire and cabaret god Ms Molly Pope)</p>
</blockquote>
<p>So in this final example we hit our ultimate goal. A case where design dependent priors are needed not because of a hacky decision process, or an awkward multivariate specification, but due to the limits of the data. In this case, priors that do not recognise the limitation of the design of the experiment will lead to poorly behaving posteriors. This manifests as the Gaussian processes severely over-fitting the data.</p>
<p>This is the ultimate expression of the point that we tried to make in the Entropy paper: <a href="http://www.stat.columbia.edu/~gelman/research/published/entropy-19-00555-v2.pdf">The prior can often only be understood in the context of the likelihood</a>.</p>
</section>
</section>
<section id="principles-can-only-get-you-so-far" class="level2">
<h2 class="anchored" data-anchor-id="principles-can-only-get-you-so-far">Principles can only get you so far</h2>
<blockquote class="blockquote">
<p>I’m making scenes, you’re constructing dioramas – <a href="https://open.spotify.com/track/4ZsjitFg4P22jukvBCSxO8?si=4c7b551f72054d92">Franklin Bruno in Nothing Painted Blue</a></p>
</blockquote>
<p>Just to round this off, I guess I should mention that the strong likelihood principle really does suggest that certain details of the design are not relevant to a fully Bayesian analysis. In particular, if the design only pops up in the normalising constant of the likelihood, it should not be relevant to a Bayesian. This seems at odds with everything I’ve said so far.</p>
<p>But it’s not.</p>
<p>In each of these cases, the design was only invoked in order to deal with some external information. For sparsity, design was needed to properly infer a sparse signal and came in through the structure of the decision process.</p>
<p>For the CAR models, the external information was that the elicitable quantity was the marginal standard deviation, which was a complicated function of the design and the standard parameter.</p>
<p>For Gaussian processes, the same thing happened: the implicit decision criterion was that we wanted to make good predictions. The design told us which parts of the parameter space obstructed this goal, and a well specified prior removed the problem.</p>
<p>There are also any number of cases in real practice where the decision at hand is stochastically dependent on the data gathering mechanism. This is why things like MRP exist.</p>
<p>I guess this is the tl;dr version of this post (because apparently I’m too wordy for some people. I suggest they read other things. Of course suggesting this in the final paragraph of such a wordy post is very me.):</p>
<p><em>Design matters even if you’re Bayesian. Especially if you want to do something with your posterior that’s more exciting than just sitting on it.</em></p>
<p><strong>Edited from an <a href="https://statmodeling.stat.columbia.edu/2017/11/05/why-wont-you-cheat-with-me/">original blog, posted November 2017</a>.</strong></p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Imagine it’s November 2017.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Again, 2017.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>2021: I am still sad War Paint closed.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>2017<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>I eventually did coincide with a Human Hearts concert and, to my extreme joy, Jenny Toomey did two songs with the band! They were supporting <a href="https://www.youtube.com/watch?v=oBwd4rAr3Rc">Gramercy Arms</a>, who I’d never heard before that night but have several perfect albums.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>This is like the standard deviation we’d use in an iid normal prior for a non-sparse model.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>2021: I did indeed type it all again. And a proof. Because why bother if you’re not going to do it well.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Yes. No open source for me!<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Abramowitz, M. and Stegun, I. (1972). Handbook of Mathematical Functions. Formula 5.1.20<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>There’s a recent paper (2021) in JRSSSB that says that these nuggets are identifiable under infill with a “nugget”, which is equivalent to observing with iid noise that magically stays independent as you observe locations closer and closer together. I will let you judge how relevant this case is to your practice. But regardless, for a <em>finite</em> set of data under any reasonable likelihood, you hit these identifiabiliy problems. And in my personal experience, they persevere even with a decent number of sites.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{simpson2021,
  author = {Dan Simpson and Dan Simpson},
  editor = {},
  title = {Why Won’t You Cheat with Me? {(Repost)}},
  date = {2021-12-09},
  url = {https://dansblog.netlify.app/2021-12-08-why-wont-you-cheat-with-me-repost/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-simpson2021" class="csl-entry quarto-appendix-citeas" role="doc-biblioentry">
Dan Simpson, and Dan Simpson. 2021. <span>“Why Won’t You Cheat with Me?
(Repost).”</span> December 9, 2021. <a href="https://dansblog.netlify.app/2021-12-08-why-wont-you-cheat-with-me-repost/">https://dansblog.netlify.app/2021-12-08-why-wont-you-cheat-with-me-repost/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>