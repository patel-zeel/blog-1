<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.15">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dan Simpson">
<meta name="dcterms.date" content="2022-06-15">
<meta name="description" content="Look. I had to do it so I wrote it out in detail. This is some of the convergence theory for truncated and winzorised importance sampling estimators">

<title>Un garçon pas comme les autres (Bayes) - Tail stabilization of importance sampling etimators: A bit of theory</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Un garçon pas comme les autres (Bayes) - Tail stabilization of importance sampling etimators: A bit of theory">
<meta property="og:description" content="Look. I had to do it so I wrote it out in detail. This is some of the convergence theory for truncated and winzorised importance sampling estimators">
<meta property="og:image" content="https://dansblog.netlify.app/posts/2022-06-03-that-psis-proof/judy.JPG">
<meta property="og:site-name" content="Un garçon pas comme les autres (Bayes)">
<meta name="twitter:title" content="Sparse Matrices 4: Design is my passion">
<meta name="twitter:description" content="Look. I had to do it so I wrote it out in detail. This is some of the convergence theory for truncated and winzorised importance sampling estimators">
<meta name="twitter:image" content="https://dansblog.netlify.app/posts/2022-06-03-that-psis-proof/judy.JPG">
<meta name="twitter:creator" content="@dan_p_simpson">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Un garçon pas comme les autres (Bayes)</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About this blog</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/dan_p_simpson"><i class="bi bi-twitter" role="img" aria-label="twitter">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/dpsimpson"><i class="bi bi-github" role="img" aria-label="github">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://dansblog.netlify.app"><i class="bi bi-person-circle" role="img" aria-label="website">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Tail stabilization of importance sampling etimators: A bit of theory</h1>
                  <div>
        <div class="description">
          <p>Look. I had to do it so I wrote it out in detail. This is some of the convergence theory for truncated and winzorised importance sampling estimators</p>
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Importance sampling</div>
                <div class="quarto-category">Computation</div>
                <div class="quarto-category">Truncated importance sampling</div>
                <div class="quarto-category">Windsorized importance sampling</div>
                <div class="quarto-category">Pareto smoothed importance sampling</div>
                <div class="quarto-category">PSIS</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Dan Simpson </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 15, 2022</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#truncated-importance-sampling" id="toc-truncated-importance-sampling" class="nav-link active" data-scroll-target="#truncated-importance-sampling">Truncated importance sampling</a>
  <ul class="collapse">
  <li><a href="#the-bias-of-tis" id="toc-the-bias-of-tis" class="nav-link" data-scroll-target="#the-bias-of-tis">The bias of TIS</a></li>
  <li><a href="#the-variance-in-tis" id="toc-the-variance-in-tis" class="nav-link" data-scroll-target="#the-variance-in-tis">The variance in TIS</a></li>
  <li><a href="#asymptotic-properties" id="toc-asymptotic-properties" class="nav-link" data-scroll-target="#asymptotic-properties">Asymptotic properties</a></li>
  </ul></li>
  <li><a href="#winsorised-importance-sampling" id="toc-winsorised-importance-sampling" class="nav-link" data-scroll-target="#winsorised-importance-sampling">Winsorised importance sampling</a>
  <ul class="collapse">
  <li><a href="#variance-of-winsorized-importance-sampling" id="toc-variance-of-winsorized-importance-sampling" class="nav-link" data-scroll-target="#variance-of-winsorized-importance-sampling">Variance of Winsorized Importance Sampling</a></li>
  </ul></li>
  <li><a href="#pareto-smoothed-importance-sampling" id="toc-pareto-smoothed-importance-sampling" class="nav-link" data-scroll-target="#pareto-smoothed-importance-sampling">Pareto-smoothed importance sampling</a></li>
  <li><a href="#final-thoughts" id="toc-final-thoughts" class="nav-link" data-scroll-target="#final-thoughts">Final thoughts</a></li>
  </ul>
</nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Imagine you have a target probability distribution <span class="math inline">\(p(\theta)\)</span> and you want to estimate the expectation <span class="math inline">\(I_h = \int h(\theta) p(\theta)\,d(\theta)\)</span>. That’s lovely and everything, but if it was easy none of us would have jobs. High-dimensional quadrature is a pain in the arse.</p>
<p>A very simple way to get an decent estimate of <span class="math inline">\(I_h\)</span> is to use <em>importance sampling</em>, that is taking draws <span class="math inline">\(\theta_s\)</span>, <span class="math inline">\(s = 1,\ldots, S\)</span> from some proposal distribution <span class="math inline">\(\theta_s \sim g(\theta)\)</span>. Then, noting that <span class="math display">\[
I_h = \int h(\theta) p (\theta)\,d\theta = \int h(\theta) \underbrace{\frac{p(\theta)}{g(\theta)}}_{r(\theta)}g(\theta)\,d\theta,
\]</span> we can use Monte Carlo to estimate the second integral. This leads to the importance sampling estimator <span class="math display">\[
I_h^S = \sum_{s=1}^S h(\theta_s) r(\theta_s).
\]</span></p>
<p>This all seems marvellous, but there is a problem. Even though <span class="math inline">\(h\)</span> is probably a very pleasant function and <span class="math inline">\(g\)</span> is a nice friendly distribution, <span class="math inline">\(r(\theta)\)</span> can be an absolute beast. Why? Well it’s<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> the ratio of two densities and there is no guarantee that the ratio of two nice functions is itself a nice function. In particular, if the bulk of the distributions <span class="math inline">\(p\)</span> and <span class="math inline">\(g\)</span> are in different places, you’ll end up with the situation where for most draws <span class="math inline">\(r(\theta_s)\)</span> is very small<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> and a few will be HUGE<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>This will lead to an extremely unstable estimator.</p>
<p>It is pretty well known that the raw importance sampler <span class="math inline">\(I_h^S\)</span> will behave nicely (that is will be unbiased with finite variance) precisely when the distribution of <span class="math inline">\(r_s = r(\theta_s)\)</span> has finite variance.</p>
<p>Elementary treatments stop there, but they miss two very big problems. The most obvious one is that it’s basically impossible to check if the variance of <span class="math inline">\(r_s\)</span> is finite. A second, much larger but much more subtle problem, is that the variance can be finite but <em>massive</em>. This is probably the most common case in high dimensions. McKay has an excellent example where the importance ratios are <em>bounded</em>, but that bound is so large that it is infinite for all intents and purposes.</p>
<p>All of which is to say that importance sampling doesn’t work unless you work on it.</p>
<section id="truncated-importance-sampling" class="level2">
<h2 class="anchored" data-anchor-id="truncated-importance-sampling">Truncated importance sampling</h2>
<p>If the problem is the fucking ratios then by gum we will fix the fucking ratios. Or so the saying goes.</p>
<p>The trick turns out to be modifying the largest ratios enough that we stabilise the variance, but not so much as to overly bias the estimate.</p>
<p>The first version of this was <a href="https://www.jstor.org/stable/27594308?seq=1">truncated importance sampling</a> (TIS), which selects a threshold <span class="math inline">\(T\)</span> and estimates the expectation as <span class="math display">\[
I_\text{TIS}^S = \frac{1}{S}\sum_{s= 1}^S h(\theta_s) \max\{r(\theta_s), T\}.
\]</span> It’s pretty obvious that <span class="math inline">\(I^S_\text{TIS}\)</span> has finite variance for any fixed <span class="math inline">\(T\)</span>, but we should be pretty worried about the bias. Unsurprisingly, there is going to be a trade-off between the variance and the bias. So let’s explore that.</p>
<section id="the-bias-of-tis" class="level3">
<h3 class="anchored" data-anchor-id="the-bias-of-tis">The bias of TIS</h3>
<p>To get an expression for the bias, first let us write <span class="math inline">\(r_s = r(\theta_s)\)</span> and <span class="math inline">\(h_s = h(\theta_s)\)</span> for <span class="math inline">\(\theta_s \sim g\)</span>. Occasionally we will talk about the joint distribution or <span class="math inline">\((r_s,h_s) \sim (R,H)\)</span>. Sometimes we will also need to use the indicator variables <span class="math inline">\(z_i = 1_{r_i &lt; T}\)</span>.</p>
<p>Then, we can write<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> <span class="math display">\[
I = \mathbb{E}(HR \mid R \leq T) \Pr(R \leq T) + \mathbb{E}(HR \mid R &gt; T) \Pr(R &gt; T).
\]</span></p>
<p>How does this related to TIS? Well. Let <span class="math inline">\(M = \sum_{s=1}^S z_i\)</span> be the random variable denoting the number of times <span class="math inline">\(r_i &gt; T\)</span>. Then, <span class="math display">\[\begin{align*}
\mathbb{E}(I_\text{TIC}^S) &amp;= \mathbb{E}\left( \frac{1}{S}\sum_{s=1}^Sz_ih_ir_i\right)  + \mathbb{E}\left( \frac{T}{S}\sum_{s=1}^S(1-z_i)h_i\right) \\
&amp;=\mathbb{E}_M\left[\frac{S-M}{S}\mathbb{E}(HR \mid R &lt; T) + \frac{MT}{S}\mathbb{E}(H \mid R &gt; T)\right] \\
&amp;=\mathbb{E}(HR \mid R \leq T) \Pr(R \leq T) + T\mathbb{E}(H \mid R &gt; T) \Pr(R &gt; T).
\end{align*}\]</span></p>
<p>Hence the bias in TIS is <span class="math display">\[
I - \mathbb{E}(I_\text{TIS}^S) = \mathbb{E}(H(R-T) \mid R &gt; T) \Pr(R &gt; T).
\]</span></p>
<p>To be honest, this doesn’t look phenomenally interesting for fixed <span class="math inline">\(T\)</span>, however if we let <span class="math inline">\(T = T_S\)</span> depend on the sample size then as long as <span class="math inline">\(T_S \rightarrow \infty\)</span> we get vanishing bias.</p>
<p>We can get more specific if we make the assumption about the tail of the importance ratios. In particular, we will assume that<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> <span class="math inline">\(1-R(r) = \Pr(R &gt; r) = cr^{-1/k}(1+o(1))\)</span> for some<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> <span class="math inline">\(k&lt;1\)</span>.</p>
<p>While it seems like this will only be useful for estimating <span class="math inline">\(\Pr(R&gt;T)\)</span>, it turns out that under some mild<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> technical conditions, the conditional excess distribution function<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> <span class="math display">\[
R_T(y) = \Pr(R - T \leq y \mid R &gt; T) = \frac{R(T + y) - R(T)}{1-R(T)},
\]</span> is well approximated by a Generalised Pareto Distribution as <span class="math inline">\(T\rightarrow \infty\)</span>. Or, in maths, as <span class="math inline">\(T\rightarrow \infty\)</span>, <span class="math display">\[
R_T(y) \rightarrow \begin{cases} 1- \left(1 + \frac{ky}{\sigma}\right)^{-1/k}, \quad &amp; k \neq 0 \\
1- \mathrm{e}^{-y/\sigma}, \quad &amp;k = 0,
\end{cases}
\]</span> for some <span class="math inline">\(\sigma &gt; 0\)</span> and <span class="math inline">\(k \in \mathbb{R}\)</span>. The shape<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> parameter <span class="math inline">\(k\)</span> is very important for us, as it tells us how many moments the distribution has. In particular, if a distribution <span class="math inline">\(X\)</span> has shape parameter <span class="math inline">\(k\)</span>, then <span class="math display">\[
\mathbb{E}|X|^\alpha &lt; \infty, \quad \forall \alpha &lt; \frac{1}{k}.
\]</span> We will focus exclusively on the case where <span class="math inline">\(k &lt; 1\)</span>. When <span class="math inline">\(k &lt; 1/2\)</span>, the distribution has finite variance.</p>
<p>If <span class="math inline">\(1- R(r) = cr^{-1/k}(1+ o(1))\)</span>, then the conditional exceedence function is <span class="math display">\[\begin{align*}
R_T(y) &amp;=  \frac{cT^{-1/k}(1+  o(1)) - c(T+y)^{-1/k}(1+  o(1))}{cT^{-1/k}(1+  o(1)))} \\
&amp;= \left[1 - \left(1 + \frac{y}{T}\right)^{-1/k}\right](1 + o(1)),
\end{align*}\]</span> which suggests that as <span class="math inline">\(T\rightarrow \infty\)</span>, <span class="math inline">\(R_T\)</span> converges to a generalised Pareto distribution with shape parameter <span class="math inline">\(k\)</span> and scale parameter <span class="math inline">\(\mathcal{O}(T)\)</span>.</p>
<p>All of this work lets us approximate the distribution of <span class="math inline">\((R-T \mid R&gt;T )\)</span> and use the formula for the mean of a generalised Pareto distribution. This gives us the estimate <span class="math display">\[
\mathbb{E}(R- T \mid R&gt;T) \approx \frac{T}{1-k},
\]</span> which estimates the bias when <span class="math inline">\(h(\theta)\)</span> is constant<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> as <span class="math display">\[
I - \mathbb{E}(I_\text{TIS}^S) \approx \mathcal{O}\left(T^{1-1/k}\right).
\]</span></p>
<p>For what it’s worth, Ionides got the same result more directly in the TIS paper, but he wasn’t trying to do what I’m trying to do.</p>
</section>
<section id="the-variance-in-tis" class="level3">
<h3 class="anchored" data-anchor-id="the-variance-in-tis">The variance in TIS</h3>
<p>The variance is a little bit more annoying. We want it to go to zero.</p>
<p>As before, we condition on <span class="math inline">\(z_s\)</span> (or, equivalently, <span class="math inline">\(M\)</span>) and then use the law of total variance. We know from the bias calculation that <span class="math display">\[
\mathbb{E}(I_\text{TIS}^S \mid M) =\frac{S-M}{S}\mathbb{E}(HR \mid R&gt;T) + \frac{TM}{S}\mathbb{E}(H \mid R&gt;T).
\]</span></p>
<p>A similarly quick calculation tells us that <span class="math display">\[
\mathbb{V}(I_\text{TIS}^S \mid M) = \frac{S-M}{S^2}\mathbb{V}(HR \mid R \leq T) +\frac{MT^2}{S^2}\mathbb{V}(H \mid R&gt;T).
\]</span> To close it out, we recall that <span class="math inline">\(M\)</span> is the sum of Bernoulli random variables so <span class="math display">\[
M \sim \text{Binomial}(S, \Pr(R &gt; T)).
\]</span></p>
<p>With this, we can get an expression for the unconditional variance. To simplify the expression, let’s write <span class="math inline">\(p_T = \Pr(R &gt; T)\)</span>. Then, <span class="math display">\[\begin{align*}
\mathbb{V}(I_\text{TIS}^S) &amp;=\mathbb{E}_M\mathbb{V}(I_\text{TIS}^S \mid M) + \mathbb{V}_M\mathbb{E}(I_\text{TIS}^S \mid M) \\
&amp;= S^{-1}(1-p_T)\mathbb{V}(HR \mid R \leq T) +S^{-1}T^2p_T\mathbb{V}(H \mid R&gt;T)\\
&amp;\quad + S^{-1}p_T(1-p_T)\mathbb{E}(HR \mid R&gt;T)^2 + S^{-1}Tp_T(1-p_T)\mathbb{E}(H \mid R&gt;T)^2.
\end{align*}\]</span></p>
<p>There are four terms in the variance. The first and third terms are clearly harmless: they go to zero no matter how we choose <span class="math inline">\(T_S\)</span>. Our problem terms are the second and fourth. We can tame the fourth term if we choose <span class="math inline">\(T_S = o(S)\)</span>. But that doesn’t seem to help with the second term. But it turns out it is enough. To see this, we note that <span class="math display">\[\begin{align*}
Tp_T\mathbb{V}(H\mid R&gt;T) &amp;=\leq Tp_T\mathbb{E}(H^2 \mid R&gt;T)\\
&amp;\leq p_T\mathbb{E}(H^2 R\mid R&gt;T) \\
&amp;\leq \mathbb{E}(H^2 R)\\
&amp;= \int h(\theta)^2 p(\theta)\,d\theta &lt; \infty.
\end{align*}\]</span> where the second inequality uses the fact that <span class="math inline">\(R&gt;T\)</span> and the third comes from the law of total probability.</p>
<p>So the TIS estimator has vanishing bias and variance as long as the truncation <span class="math inline">\(T_S \rightarrow \infty\)</span> and <span class="math inline">\(T_S = o(S)\)</span>. Once again, this is in the TIS paper, where it is proved in a much more compact way.</p>
</section>
<section id="asymptotic-properties" class="level3">
<h3 class="anchored" data-anchor-id="asymptotic-properties">Asymptotic properties</h3>
<p>It can also be useful to have an understanding of how wild the fluctuations <span class="math inline">\(I - I_\text{TIS}^S\)</span> are. For traditional importance sampling, we know that if <span class="math inline">\(\mathbb{E}(R^2)\)</span> is finite, then then the fluctuations are, asymptotically, normally distributed with mean zero. Non-asymptotic results were given by <a href="https://arxiv.org/abs/1511.01437">Chatterjee and Diaconis</a> that also hold even when the estimator has infinite variance.</p>
<p>For TIS, it’s pretty obvious that for fixed <span class="math inline">\(T\)</span> and <span class="math inline">\(h \geq 0\)</span>, <span class="math inline">\(I_\text{TIS}^S\)</span> will be asymptotically normal (it is, after all, the sum of bounded random variables). For growing sequences <span class="math inline">\(T_S\)</span> it’s a tiny bit more involved: it is now a triangular array<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> rather than a sequence of random variables. But in the end very classical results tell us that for bounded<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a> <span class="math inline">\(h\)</span>, the fluctuations of the TIS estimator are asymptotically normal.</p>
<p>It’s worth saying that when <span class="math inline">\(h(\theta)\)</span> is unbounded, it <em>might</em> be necessary to truncate the product <span class="math inline">\(h_ir_i\)</span> rather than just <span class="math inline">\(r_i\)</span>. This is especially relevant if <span class="math inline">\(\mathbb{E}(H \mid R=r)\)</span> grows rapidly with <span class="math inline">\(r\)</span>. Personally, I can’t think of a case where this happens: <span class="math inline">\(r(\theta)\)</span> usually grows (super-)exponentially in <span class="math inline">\(\theta\)</span> while <span class="math inline">\(h(\theta)\)</span> usually grows polynomially, which implies <span class="math inline">\(\mathbb{E}(H \mid R=r)\)</span> grows (poly-)logarithmically.</p>
<p>The other important edge case is that when <span class="math inline">\(h(\theta)\)</span> can be both positive and negative, it might be necessary to truncate <span class="math inline">\(h_ir_i\)</span> both above <em>and</em> below.</p>
</section>
</section>
<section id="winsorised-importance-sampling" class="level2">
<h2 class="anchored" data-anchor-id="winsorised-importance-sampling">Winsorised importance sampling</h2>
<p>TIS has lovely theoretical properties, but it’s a bit challenging to use in practice. The problem is, there’s really no practical guidance on how to choose the truncation sequence.</p>
<p>So let’s do this differently. What if instead of specifying a threshold directly, we instead decided that the largest <span class="math inline">\(M\)</span> values are potentially problematic and should be modified? Recall that for TIS, the number of samples that exceeded the threshold, <span class="math inline">\(M\)</span>, was random while the threshold was fixed. This is the opposite situation: the number of exceedences is fixed but the threshold is random.</p>
<p>The threshold is now the <span class="math inline">\(M\)</span>th largest value of <span class="math inline">\(r_s\)</span>. We denote this using order statistics notation: we re-order the sample so that <span class="math display">\[
r_{1:S} \leq r_{2:S}\leq \ldots r_{S:S}.
\]</span> With this notation, the threshold is <span class="math inline">\(T = r_{S-M+1:S}\)</span> and the Winsorized importance sampler (WIS) is <span class="math display">\[
I^S_\text{WIS} = \frac{1}{S}\sum_{s = 1}^{S-M} h_{s:S}r_{s:S} + \frac{r_{S-M+1:S}}{S}\sum_{s=S-M+1}^S h_{s:S},
\]</span> where <span class="math inline">\((r_{s:S}, h_{s:S})\)</span> are the <span class="math inline">\((r_s, h_s)\)</span> pairs <em>ordered</em> so that <span class="math inline">\(r_{1:S} \leq r_{2:S}\leq \cdots \leq r_{S:S}\)</span>. Note that <span class="math inline">\(h_{s:S}\)</span> are not necessarily in increasing order: they are known as <em>concomitants</em> of <span class="math inline">\(r_{s:S}\)</span>, which is just a fancy way to say that they’re along for the ride. It’s <em>very</em> important that we reorder the <span class="math inline">\(h_s\)</span> when we reorder the <span class="math inline">\(r_s\)</span>, otherwise we won’t preserve the joint distribution and we’ll end up with absolute rubbish.</p>
<p>We can already see that this is both much nicer and much wilder than the TIS distribution. It is <em>convenient</em> that <span class="math inline">\(M\)</span> is no longer random! But what the hell are we going to do about those order statistics? Well, the answer is very much the same thing as before: condition on them and hope for the best.</p>
<p>Conditioned on the event<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a> <span class="math inline">\(\{r_{S-M+1:S} = T\}\)</span>, we get <span class="math display">\[
\mathbb{E}\left(I_\text{WIS}^S \mid r_{S-M+1:S} = T\right) = \left(1 - \frac{M}{S}\right)\mathbb{E}(RH \mid R &lt; T) + \frac{MT}{S} \mathbb{E}(H \mid R \geq T).
\]</span> From this, we get that the bias, conditional on <span class="math inline">\(r_{S-M+1:S} = T\)</span> is <span class="math display">\[\begin{multline*}
\left|I - \mathbb{E}\left(I_\text{WIS}^S \mid r_{S-M+1:S} = T\right)\right| =\left|\left[\Pr(R &lt; T) - \left(1 - \frac{M}{S}\right)\right]\mathbb{E}(RH \mid R &lt; T) \right.\\
\left.+ \left[\Pr(R \geq T) - \frac{M}{S}\right] \mathbb{E}(H(R - T) \mid R \geq T)\right|.
\end{multline*}\]</span></p>
<p>You should immediately notice that we are in quite a different situation from TIS, where only the tail contributed to the bias. By fixing <span class="math inline">\(M\)</span> and randomising the threshold, we have bias contributions from both the bulk (due, essentially, to a weighting error) and from the tail (due to both the weighting error and the truncation). This is going to require us to be a bit creative.</p>
<p>We could probably do something more subtle and clever here, but that is not my way. Instead, let’s use the triangle inequality to say <span class="math display">\[
\left|\mathbb{E}(RH \mid R &gt; T)\right| \leq \frac{\mathbb{E}(R |H| 1(R&lt;T))}{\Pr(R &lt;T)} \leq \frac{\|h\|_{L^1(p)}}{\Pr(R  &lt;T)}
\]</span> and so the first term in the bias can be bounded if we can bound the relative error <span class="math display">\[
\mathbb{E}\left|1 - \frac{1- M/S}{\Pr(R &lt; r_{S-M+1:S})}\right|.
\]</span></p>
<p>Now the more sensible among you will say <em><a href="https://www.youtube.com/watch?v=R-HryG35A2E">Daniel, No!</a> That’s a ratio! That’s going to be hard to bound</em>. And, of course, you are right. But here’s the thing: if <span class="math inline">\(M\)</span> is small relative to <span class="math inline">\(S\)</span>, it is <em>tremendously</em> unlikely that <span class="math inline">\(r_{S-M+1:S}\)</span> is anywhere near zero. This is intuitively true, but also mathematically true.</p>
<p>To attack this expectation, we are going to look at a slightly different quantity that has the good grace of being non-negative.</p>
<div id="lem-lem1" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1 </strong></span>Let <span class="math inline">\(X_s\)</span>, <span class="math inline">\(s= 1, \ldots S\)</span> be an iid sample from <span class="math inline">\(F_X\)</span>, let <span class="math inline">\(0\leq k\leq S\)</span> be an integer. Then <span class="math display">\[
\frac{p}{F_X(x_{k:S})} -p \stackrel{d}{=} \frac{p(S-k+1)}{k} \mathcal{F},
\]</span> and <span class="math display">\[
\frac{1-p}{1- F_x/(x_{k:S})} - (1-p) \stackrel{d}{=} \frac{k(1-p)}{S-k+1}\mathcal{F}^{-1}
\]</span> where <span class="math inline">\(\mathcal{F}\)</span> is an F-distributed random variable with parameters <span class="math inline">\((2(S-k+1), 2k)\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>For any <span class="math inline">\(t\geq 0\)</span>, <span class="math display">\[\begin{align*}
\Pr\left(\frac{p}{F_X(x_{k:S})} - p \leq t\right) &amp;=\Pr\left(p - pF_X(x_{k:S}) \leq tF_X(x_{k:S})\right) \\
&amp;= \Pr\left(p  \leq (t+p)F_X(x_{k:S})\right) \\
&amp;=\Pr\left(F_X(x_{k:S}) \geq \frac{p}{p+t}\right)\\
&amp;= \Pr\left(x_{k:S} \geq F_X^{-1}\left(\frac{p}{p+t}\right)\right)\\
&amp;= 1- I_{\frac{p}{p+t}}(k, S-k+1) \\
&amp;= I_{\frac{t}{p+t}}(S-k+1, k),
\end{align*}\]</span> where <span class="math inline">\(I_p(a,b)\)</span> is the incomplete Beta function.</p>
<p>You could, quite reasonably, ask where the hell that incomplete Beta function came from. And if I had thought to look this up, I would say that it came from Equation 2.1.5 in David and Nagaraja’s book on order statistics. Unfortunately, I did not look this up. I derived it, which is honestly not very difficult. The trick is to basically note that the event <span class="math inline">\(\{x_{k:S} \leq \tau\}\)</span> is the same as the event that at least <span class="math inline">\(k\)</span> of the samples <span class="math inline">\(x_s\)</span> are less than or equal to <span class="math inline">\(\tau\)</span>. Because the <span class="math inline">\(x_s\)</span> are independent, this is the probability of observing at least <span class="math inline">\(k\)</span> heads from a coin with the probability of a head <span class="math inline">\(\Pr(x \leq \tau) = F_X(\tau)\)</span>. If you look this up on Wikipedia<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> you see<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a> that it is <span class="math inline">\(I_{1-F_X(\tau)}(k,S-k+1)\)</span>. The rest just come from noting that <span class="math inline">\(\tau = F_X^{-1}(t/(p+t))\)</span> and using the symmetry <span class="math inline">\(1-I_p(a,b) = I_{1-p}(b,a)\)</span>.</p>
<p>To finish this off, we note that <span class="math display">\[
\Pr(\mathcal{F} \leq x) = I_{\frac{S-k+1}{(S-k+1)x+ k}}(S-k+1,k).
\]</span> From which, we see that <span class="math display">\[\begin{align*}
\Pr\left(\frac{p}{F_X(x_{k:S})} - p \leq t\right) &amp;=\Pr\left(\mathcal{F} \leq \frac{k}{p(S-k+1)}t\right) \\
&amp;= \Pr\left(\frac{p(S-k+1)}{k}\mathcal{F} \leq t\right).
\end{align*}\]</span></p>
<p>The second result follows the same way and by noting that <span class="math inline">\(\mathcal{F}^{-1}\)</span> is also F-distributed with parameters <span class="math inline">\((k, S-k+1)\)</span>.</p>
<p><em>The proof has ended</em></p>
</div>
<p>Now, obviously, in this house we do not trust mathematics. Which is to say that I made a stupid mistake the first time I did this and forgot that when <span class="math inline">\(Z\)</span> is binomial, <span class="math inline">\(\Pr(Z \geq k) = 1 - \Pr(Z \leq k-1)\)</span> and had a persistent off-by-one error in my derivation. But we test out our results so we don’t end up doing the dumb thing.</p>
<p>So let’s do that. For this example, we will use generalised Pareto-distributed <span class="math inline">\(X\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-2"><a href="#cb1-2"></a>xi <span class="ot">&lt;-</span> <span class="fl">0.7</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>s <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>u <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a>samp <span class="ot">&lt;-</span> <span class="cf">function</span>(S, k, p, </span>
<span id="cb1-7"><a href="#cb1-7"></a>                 <span class="at">Q =</span> \(x) u <span class="sc">+</span> s<span class="sc">*</span>((<span class="dv">1</span><span class="sc">-</span>x)<span class="sc">^</span>(<span class="sc">-</span>xi)<span class="sc">-</span><span class="dv">1</span>)<span class="sc">/</span>xi, </span>
<span id="cb1-8"><a href="#cb1-8"></a>                 <span class="at">F =</span> \(x) <span class="dv">1</span> <span class="sc">-</span> (<span class="dv">1</span> <span class="sc">+</span> xi<span class="sc">*</span>(x <span class="sc">-</span> u)<span class="sc">/</span>s)<span class="sc">^</span>(<span class="sc">-</span><span class="dv">1</span><span class="sc">/</span>xi)) {</span>
<span id="cb1-9"><a href="#cb1-9"></a>  <span class="co"># Use theory to draw x_{k:S}</span></span>
<span id="cb1-10"><a href="#cb1-10"></a>  xk <span class="ot">&lt;-</span> <span class="fu">Q</span>(<span class="fu">rbeta</span>(<span class="dv">1</span>, k, S <span class="sc">-</span> k <span class="sc">+</span> <span class="dv">1</span>))</span>
<span id="cb1-11"><a href="#cb1-11"></a>  <span class="fu">c</span>(<span class="dv">1</span> <span class="sc">-</span> p <span class="sc">/</span> <span class="fu">F</span>(xk), <span class="dv">1</span><span class="sc">-</span>(<span class="dv">1</span><span class="sc">-</span>p)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">F</span>(xk)))</span>
<span id="cb1-12"><a href="#cb1-12"></a>}</span>
<span id="cb1-13"><a href="#cb1-13"></a></span>
<span id="cb1-14"><a href="#cb1-14"></a>S <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb1-15"><a href="#cb1-15"></a>M <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb1-16"><a href="#cb1-16"></a>k <span class="ot">&lt;-</span> S <span class="sc">-</span> M <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb1-17"><a href="#cb1-17"></a>p <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span>M<span class="sc">/</span>S</span>
<span id="cb1-18"><a href="#cb1-18"></a>N <span class="ot">&lt;-</span> <span class="dv">100000</span></span>
<span id="cb1-19"><a href="#cb1-19"></a></span>
<span id="cb1-20"><a href="#cb1-20"></a>fs <span class="ot">&lt;-</span> <span class="fu">rf</span>(N, <span class="dv">2</span> <span class="sc">*</span> (S <span class="sc">-</span> k <span class="sc">+</span> <span class="dv">1</span>), <span class="dv">2</span> <span class="sc">*</span> k )</span>
<span id="cb1-21"><a href="#cb1-21"></a><span class="fu">tibble</span>(<span class="at">theoretical =</span> <span class="dv">1</span><span class="sc">-</span>p <span class="sc">-</span> p <span class="sc">*</span> fs <span class="sc">*</span> (S <span class="sc">-</span> k <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">/</span>k,</span>
<span id="cb1-22"><a href="#cb1-22"></a>       <span class="at">xks =</span> <span class="fu">map_dbl</span>(<span class="dv">1</span><span class="sc">:</span>N, \(x) <span class="fu">samp</span>(S, k, p)[<span class="dv">1</span>])) <span class="sc">%&gt;%</span></span>
<span id="cb1-23"><a href="#cb1-23"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">stat_ecdf</span>(<span class="fu">aes</span>(<span class="at">x =</span> xks), <span class="at">colour =</span> <span class="st">"black"</span>) <span class="sc">+</span> </span>
<span id="cb1-24"><a href="#cb1-24"></a>  <span class="fu">stat_ecdf</span>(<span class="fu">aes</span>(<span class="at">x =</span> theoretical), <span class="at">colour =</span> <span class="st">"red"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb1-25"><a href="#cb1-25"></a>  <span class="fu">ggtitle</span>(<span class="fu">expression</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fu">frac</span>(<span class="dv">1</span><span class="sc">-</span>M<span class="sc">/</span>S , <span class="fu">R</span>(r[S<span class="sc">-</span>M<span class="sc">+</span><span class="dv">1</span><span class="sc">:</span>S]))))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="that-psis-proof_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="fu">tibble</span>(<span class="at">theoretical =</span> p <span class="sc">-</span> (<span class="dv">1</span><span class="sc">-</span>p) <span class="sc">*</span> k<span class="sc">/</span>(fs <span class="sc">*</span> (S <span class="sc">-</span> k <span class="sc">+</span> <span class="dv">1</span>)),</span>
<span id="cb2-2"><a href="#cb2-2"></a>       <span class="at">xks =</span> <span class="fu">map_dbl</span>(<span class="dv">1</span><span class="sc">:</span>N, \(x) <span class="fu">samp</span>(S, k, p)[<span class="dv">2</span>])) <span class="sc">%&gt;%</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> <span class="fu">stat_ecdf</span>(<span class="fu">aes</span>(<span class="at">x =</span> xks), <span class="at">colour =</span> <span class="st">"black"</span>) <span class="sc">+</span> </span>
<span id="cb2-4"><a href="#cb2-4"></a>  <span class="fu">stat_ecdf</span>(<span class="fu">aes</span>(<span class="at">x =</span> theoretical), <span class="at">colour =</span> <span class="st">"red"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb2-5"><a href="#cb2-5"></a>  <span class="fu">ggtitle</span>(<span class="fu">expression</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fu">frac</span>(M<span class="sc">/</span>S , <span class="dv">1</span><span class="sc">-</span><span class="fu">R</span>(r[S<span class="sc">-</span>M<span class="sc">+</span><span class="dv">1</span><span class="sc">:</span>S]))))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="that-psis-proof_files/figure-html/unnamed-chunk-1-2.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Fabulous. It follow then that <span class="math display">\[
\left|1 - \frac{1-M/S}{R(r_{S-M+1})} \right| \stackrel{d}= \left|\frac{M}{S} -  \frac{M(S-M)}{S(S-M-1)}\mathcal{F}\right| \leq \frac{M}{S} +  \frac{M(S-M)}{S(S-M-1)} \mathcal{F},
\]</span> where <span class="math inline">\(\mathcal{F}\)</span> has an F-distribution with <span class="math inline">\((M, S-M+1)\)</span> degrees of freedom. As <span class="math inline">\(\mathbb{E}(\mathcal{F}) = 1 + 1/(S-M-1)\)</span>, it follows that this term goes to zero as long as <span class="math inline">\(M = o(S)\)</span>. This shows that the first term in the bias goes to zero.</p>
<p>It’s worth noting here that we’ve also calculated that the bias is <em>at most</em> <span class="math inline">\(\mathcal{O}(M/S)\)</span>, however, this rate is extremely sloppy. That upper bound we just computed is <em>unlikely</em> to be tight. A better person than me would probably check, but honestly I just don’t give a shit<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></p>
<p>The second term in the bias is <span class="math display">\[
\left[\Pr(R \geq T) - \frac{M}{S}\right] \mathbb{E}(H(R - T) \mid R \geq T).
\]</span> As before, we can write this as <span class="math display">\[
\left(1 - \frac{M/S}{1-R(T)}\right)|\mathbb{E}(H(R - T) 1_{R \geq T})| \leq \left|1 - \frac{M/S}{1-R(T)}\right|\|h\|_{L^1(p)}.
\]</span> By our lemma, we know that the distribution of the term in the absolute value when <span class="math inline">\(T = r_{S-M+1}\)</span> is the same as <span class="math display">\[
1-\frac{M}{S} -\left(1 - \frac{M}{S} + \frac{1}{S}\right)\mathcal{F} = (\mu_F-\mathcal{F})  +\frac{M}{S}(\mathcal{F}-\mu_F) - \frac{1}{S}\mathcal{F} +  \frac{1}{M-1}\left(\frac{M}{S} - 1\right),
\]</span> where <span class="math inline">\(\mathcal{F} \sim \text{F}_{2(S-M+1), 2M}\)</span>, which has mean <span class="math inline">\(\mu_F = 1+(M-1)^{-1}\)</span> and variance <span class="math display">\[
\sigma^2_F = \frac{M^2S}{(S-M+1)(M-1)^2(M-2)} = \frac{1}{M}(1 + \mathcal{O}(M^{-1} + MS^{-1}).
\]</span> From Jensen’s inequality, we get <span class="math display">\[
\mathbb{E}(|\mathcal{F} - \mu_F|) \leq \sigma_F = M^{-1/2}(1 + o(1)).
\]</span> If follows that <span class="math display">\[
\mathbb{E}\left|1 - \frac{M/S}{1-R(r_{S-M+1:S})}\right| \leq M^{-1/2}(1+o(1))M^{1/2}S^{-1}(1 + o(1)) + S^{-1}(1+ o(1)) + (M-1)^{-1}(1+o(1)),
\]</span> and so we get vanishing bias as long as <span class="math inline">\(M\rightarrow \infty\)</span> and <span class="math inline">\(M/S \rightarrow 0\)</span>.</p>
<p>Once again, I make no claims of tightness<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a>. Just because it’s a bit sloppy at this point doesn’t mean the job isn’t done.</p>
<div id="thm-thm1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 </strong></span>Let <span class="math inline">\(\theta_s\)</span>, <span class="math inline">\(s = 1,\ldots, S\)</span> be an iid sample from <span class="math inline">\(G\)</span> and let <span class="math inline">\(r_s = r(\theta_s) \sim R\)</span>. Assume that</p>
<ol type="1">
<li><p><span class="math inline">\(R\)</span> is absolutely continuous</p></li>
<li><p><span class="math inline">\(M \rightarrow \infty\)</span> and <span class="math inline">\(S^{-1}M \rightarrow 0\)</span></p></li>
<li><p><span class="math inline">\(h \in L^1(p)\)</span></p></li>
</ol>
<p>Then Winsorized importance sampling converges in <span class="math inline">\(L^1\)</span> and is asymptotically unbiased.</p>
</div>
<p>Ok so that’s nice. But you’ll notice that I did not mention our piss-poor rate. That’s because there is absolutely no way in hell that the bias is <span class="math inline">\(\mathcal{O}(M^{-1/2})\)</span>! That rate is an artefact of a <em>very</em> sloppy bound on <span class="math inline">\(\mathbb{E}|1-\mathcal{F}|\)</span>.</p>
<p>Unfortunately, Mathematica couldn’t help me out. Its asymptotic abilities shit the bed at the sight of <span class="math inline">\({}_2F_1(a,b;c;z))\)</span>, which is everywhere in the exact expression (which I’ve put below in the fold.</p>
<details>
<summary>
Mathematica expression for <span class="math inline">\(\mathbb{E}|1-\mathcal{F}|\)</span>.
</summary>
<pre><code>-(((M/(1 + S))^(-(1/2) - S/2)*Gamma[(1 + S)/2]*
     (6*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) - 
        5*M*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) + 
        M^2*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) + 
        8*S*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) - 
        6*M*S*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) + 
        M^2*S*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) + 
        2*S^2*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) - 
        M*S^2*(M/(1 + S))^(1/2 + M/2 + S/2)*((1 + S)/(1 - M + S))^(M/2 + S/2) - 
         6*Sqrt[-(M/(-1 + M - S))]*Sqrt[(-1 - S)/(-1 + M - S)]*
        (M/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[1, (1/2)*(-1 + M - S), 
                                                      M/2, M/(-1 + M - S)] + 8*M*Sqrt[-(M/(-1 + M - S))]*
        Sqrt[(-1 - S)/(-1 + M - S)]*(M/(1 - M + S))^(M/2 + S/2)*
        Hypergeometric2F1[1, (1/2)*(-1 + M - S), M/2, M/(-1 + M - S)] - 
        2*M^2*Sqrt[-(M/(-1 + M - S))]*Sqrt[(-1 - S)/(-1 + M - S)]*
        (M/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[1, (1/2)*(-1 + M - S), 
                                                      M/2, M/(-1 + M - S)] - 8*Sqrt[-(M/(-1 + M - S))]*
        Sqrt[(-1 - S)/(-1 + M - S)]*S*(M/(1 - M + S))^(M/2 + S/2)*
        Hypergeometric2F1[1, (1/2)*(-1 + M - S), M/2, M/(-1 + M - S)] + 
        4*M*Sqrt[-(M/(-1 + M - S))]*Sqrt[(-1 - S)/(-1 + M - S)]*S*
        (M/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[1, (1/2)*(-1 + M - S), 
                                                      M/2, M/(-1 + M - S)] - 2*Sqrt[-(M/(-1 + M - S))]*
        Sqrt[(-1 - S)/(-1 + M - S)]*S^2*(M/(1 - M + S))^(M/2 + S/2)*
        Hypergeometric2F1[1, (1/2)*(-1 + M - S), M/2, M/(-1 + M - S)] + 
        6*M*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^(M/2 + S/2)*
        Hypergeometric2F1[(1 + S)/2, (1/2)*(1 - M + S), (1/2)*(3 - M + S), 
                          (-1 + M - S)/M] - 5*M^2*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^
        (M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, (1/2)*(1 - M + S), 
                                      (1/2)*(3 - M + S), (-1 + M - S)/M] + M^3*(M/(1 + S))^(M/2)*
        ((1 + S)/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, 
                                                            (1/2)*(1 - M + S), (1/2)*(3 - M + S), (-1 + M - S)/M] + 
        2*M*S*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^(M/2 + S/2)*
        Hypergeometric2F1[(1 + S)/2, (1/2)*(1 - M + S), (1/2)*(3 - M + S), 
                          (-1 + M - S)/M] - M^2*S*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^
        (M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, (1/2)*(1 - M + S), 
                                      (1/2)*(3 - M + S), (-1 + M - S)/M] - 2*M*(M/(1 + S))^(M/2)*
        ((1 + S)/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, 
                                                            (1/2)*(3 - M + S), (1/2)*(5 - M + S), (-1 + M - S)/M] + 
        3*M^2*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^(M/2 + S/2)*
        Hypergeometric2F1[(1 + S)/2, (1/2)*(3 - M + S), (1/2)*(5 - M + S), 
                          (-1 + M - S)/M] - M^3*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^
        (M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, (1/2)*(3 - M + S), 
                                      (1/2)*(5 - M + S), (-1 + M - S)/M] - 2*M*S*(M/(1 + S))^(M/2)*
        ((1 + S)/(1 - M + S))^(M/2 + S/2)*Hypergeometric2F1[(1 + S)/2, 
                                                            (1/2)*(3 - M + S), (1/2)*(5 - M + S), (-1 + M - S)/M] + 
        M^2*S*(M/(1 + S))^(M/2)*((1 + S)/(1 - M + S))^(M/2 + S/2)*
        Hypergeometric2F1[(1 + S)/2, (1/2)*(3 - M + S), (1/2)*(5 - M + S), 
                          (-1 + M - S)/M]))/(((1 + S)/(1 - M + S))^S*
                                               (2*(-2 + M)*M*Sqrt[(-1 - S)/(-1 + M - S)]*Gamma[M/2]*
                                                  Gamma[(1/2)*(5 - M + S)])))</code></pre>
</details>
<p>But do not fear: we can recover. At the cost of an assumption about the tails of <span class="math inline">\(R\)</span>. (We’re also going to assume that <span class="math inline">\(h\)</span> is bounded because it makes things ever so slightly easier, although unbounded <span class="math inline">\(h\)</span> is ok<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a> as long as it doesn’t grow too quickly relative to <span class="math inline">\(r\)</span>.)</p>
<p>We are going to make the assumption that <span class="math inline">\(R - T \mid R\geq T\)</span> is in the domain of attraction of a generalized Pareto distribution with shape parameter <span class="math inline">\(k\)</span>. A sufficient condition, due to von Mises, is that <span class="math display">\[
\lim_{r\rightarrow \infty} \frac{r R'(r)}{1-R(r)} = \frac{1}{k}.
\]</span></p>
<p>This seems like a weird condition, but it’s basically just a regularity condition at infinity. For example if <span class="math inline">\(1-R(r)\)</span> is regularly varying at infinity<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> and <span class="math inline">\(R'(r)\)</span> is, eventually, monotone<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a> decreasing, then this condition holds.</p>
<p>The von Mises condition is very natural for us as <a href="https://projecteuclid.org/journals/annals-of-probability/volume-21/issue-3/Von-Mises-Conditions-Revisited/10.1214/aop/1176989120.full">Falk and Marohn (1993)</a> show that the relative error we get when approximating the tail of <span class="math inline">\(R\)</span> by a generalised Pareto density is the same as the relative error in the von Mises condition. That is if <span class="math display">\[
\frac{rR'(r)}{1-R(r)} = \frac{1}{k}(1 + \mathcal{O}(r^{-\alpha}))
\]</span> then <span class="math display">\[
R'(r) = c w(cr - d)(1 + \mathcal{O}(r^{-\alpha})),
\]</span> where <span class="math inline">\(c,d\)</span> are constants and <span class="math inline">\(w\)</span> is the density of a generalised Pareto distribution.</p>
<p>Anyway, under those two assumptions, we can swap out the density of <span class="math inline">\((R-T)\mid R&gt;T\)</span> with its asymptotic approximation and get that, conditional on <span class="math inline">\(T= r_{S-M+1:S}\)</span>, <span class="math display">\[
\mathbb{E}(H(R-T) \mid R&gt;T) = (k-1)^{-1}T.
\]</span></p>
<p>Hence, the second term in the bias goes to zero if <span class="math display">\[
\mathbb{E}\left(r_{S-M+1:S}\left(1 - R(r_{s-M+1:S}) - \frac{M}{S}\right)\right)
\]</span> goes to zero.</p>
<p>Now this is not particularly pleasant, but it helps to recognise that even if a distribution doesn’t have finite moments, away from the extremes, its order statistics always do. This means that we can use Cauchy-Schwartz to get <span class="math display">\[
\left|\mathbb{E}\left(r_{S-M+1:S}\left(1 - R(r_{s-M+1:S}) - \frac{M}{S}\right)\right)\right| \leq\mathbb{E}\left(r_{S-M+1:S}^2\right)^{1/2}\mathbb{E}\left[\left(1 - R(r_{s-M+1:S}) - \frac{M}{S}\right)^2\right]^{1/2}.
\]</span></p>
<p>Arguably, the most alarming term is the first one, but that can<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a> be tamed. To do this, we lean into a result from <a href="https://projecteuclid.org/proceedings/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fifth-Berkeley-Symposium-on-Mathematical-Statistics-and/Chapter/Some-contributions-to-the-theory-of-order-statistics/bsmsp/1200513012">Bickel (1967)</a> who, if you examine the proof and translate some obscurely-stated conditions and fix a typo<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>, you get that <span class="math display">\[
\mathbb{E}(r_{k:M}^2) \leq C k\begin{pmatrix} S \\ k\end{pmatrix} \int_0^1 t^{k-2-1}(1-t)^{S-k-2}\,dt.
\]</span> You might worry that this is going to grow too quickly. But it doesn’t. Noting that <span class="math inline">\(B(n,m) = \Gamma(n)\Gamma(m)/\Gamma(n+m)\)</span>, we can rewrite the upper bound in terms of the Beta function to get <span class="math display">\[
\mathbb{E}(r_{k:M}^2) \leq C \frac{\Gamma(S+1)}{\Gamma(S-3)} \frac{\Gamma(k-2)}{\Gamma(k+1)}\frac{\Gamma(S-k-1)}{\Gamma(S-k+1)}.
\]</span></p>
<p>To show that this doesn’t grow too quickly, we use the identity <span class="math display">\[
\frac{\Gamma(x + a)}{\Gamma(x + b)} \propto x^{a-b}(1 + \mathcal{O}(x^{-1})).
\]</span> From this, it follows that <span class="math display">\[
\mathbb{E}(r_{k:M}^2) \leq C S^4k^{-3}(S-k)^{-2}(1+ \mathcal{O}(S^{-1}))(1+ \mathcal{O}(k^{-1}))(1+ \mathcal{O}((S+k)^{-1})).
\]</span> In this case, we are interested in <span class="math inline">\(k = S-M+1\)</span>, so <span class="math display">\[
\mathbb{E}(r_{k:M}^2) \leq C S^4S^{-3}M^{-2}(1 - M/S + 1/S)^{-3}(1 - 1/M)^{-2}(1+ \mathcal{O}(S^{-1}))(1+ \mathcal{O}(S^{-1}))(1+ \mathcal{O}(M^{-1})).
\]</span></p>
<p>Hence the we get that <span class="math inline">\(\mathbb{E}(r_{k:M}^2) = \mathcal{O}(SM^{-2})\)</span>. This is increasing<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a> in <span class="math inline">\(S\)</span>, but we will see that it is not going up too fast.</p>
<p>For the second half of this shindig, we are going to attack <span class="math display">\[
\mathbb{E}\left[\left(1 - R(r_{s-M+1:S}) - \frac{M}{S}\right)^2\right] = \mathbb{E}\left[\left(1 - R(r_{s-M+1:S})\right)^2 - 2\left(1 - R(r_{s-M+1:S})\right)\frac{M}{S} +\left(\frac{M}{S}\right)^2\right].
\]</span> A standard result<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a> from extreme value theory is that <span class="math inline">\(R(r_{k:S})\)</span> has the same distribution as the <span class="math inline">\(k\)</span>th order statistics from a sample of <span class="math inline">\(S\)</span> iid <span class="math inline">\(\text{Uniform}([0,1])\)</span> random variables. Hence<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a>, <span class="math display">\[
R(r_{S-M+1:S}) \sim \text{Beta}(S-M+1, M).
\]</span> If follows<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a> that <span class="math display">\[
\mathbb{E}(1- R(r_{S-M+1:S})) = \frac{M}{S+1} = \frac{M}{S}\frac{1}{1+S^{-1}}
\]</span> and <span class="math display">\[
\mathbb{E}((1- R(r_{S-M+1:S}))^2) = \frac{M(M+1)}{(S+1)(S+2)} = \frac{M^2}{S^2}\left(\frac{1 + M^{-1}}{1 + 3S^{-1} + 2S^{-2}}\right).
\]</span> Adding these together and doing some asymptotic expansions, we get <span class="math display">\[
\mathbb{E}\left[\left(1 - R(r_{s-M+1:S}) - \frac{M}{S}\right)^2\right] = \frac{M^2}{S^2} + \mathcal{O}\left(\frac{M}{S^2}\right),
\]</span> which goes to zero<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a> like <span class="math inline">\(\mathcal{O}(S^{-1})\)</span> if <span class="math inline">\(M = \mathcal{O}(S^{1/2})\)</span>.</p>
<p>We can multiply this rate together and get that the second term in the bias is bounded above by <span class="math display">\[
\left[\left(\frac{S}{M^2} (1 + \mathcal{O}(M^{-1} + MS^{-1}))\right)\left(\frac{M^2}{S^2} (1 + \mathcal{O}(M^{-1} + MS^{-1})\right)\right]^{1/2} = S^{-1/2}(1 + o(1)).
\]</span></p>
<p>Putting all of this together we have proved the following Corollary.</p>
<div id="cor-cor1" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 1 </strong></span>Let <span class="math inline">\(\theta_s\)</span>, <span class="math inline">\(s = 1,\ldots, S\)</span> be an iid sample from <span class="math inline">\(G\)</span> and let <span class="math inline">\(r_s = r(\theta_s) \sim R\)</span>. Assume that</p>
<ol type="1">
<li><p><span class="math inline">\(R\)</span> is absolutely continuous and satisfies the von Mises condition<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a> <span class="math display">\[
\frac{rR'(r)}{1-R(r)} = \frac{1}{k}(1 +\mathcal{O}(r^{-1})).
\]</span></p></li>
<li><p><span class="math inline">\(M = o(S)\)</span></p></li>
<li><p><span class="math inline">\(h\)</span> is bounded<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a></p></li>
</ol>
<p>Winsorized importance sampling converges in <span class="math inline">\(L^1\)</span> with rate of, at most, <span class="math inline">\(\mathcal{O}(MS^{-1} + S^{-1/2})\)</span>, which is balanced when <span class="math inline">\(M = \mathcal{O}(S^{1/2})\)</span>. Hence, WIS is<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a> <span class="math inline">\(\sqrt{n}\)</span>-consistent.</p>
</div>
<section id="variance-of-winsorized-importance-sampling" class="level3">
<h3 class="anchored" data-anchor-id="variance-of-winsorized-importance-sampling">Variance of Winsorized Importance Sampling</h3>
<p>Right, that was a bit of a journey, but let’s keep going to the variance.</p>
<p>It turns out that following the route I thought I was going to follow does not end well. That lovely set of tricks breaking up the variance into two conditional terms turns out to be very very unnecessary. Which is good, because I thoroughly failed to make the argument work.</p>
<p>If you’re curious, the problem is that the random variable <span class="math display">\[
\frac{Mr_{S-M+1:S}}{S} \mathbb{E}(H \mid R \geq r_{S-M+1:S}) = \frac{Mr_{S-M+1:S}}{S(1-R(r_{S-M+1:S}))} \mathbb{E}(H 1_{R \geq r_{S-M+1:S}})
\]</span> is an absolute <em>bastard</em> to bound. The problem is that <span class="math inline">\(1- R({r_{S-M+1:S}}) \approx M/S\)</span> and so the usual trick of bounding that truncated expectation by <span class="math inline">\(\|h\|\)</span> or some such thing will prove that the variance is <em>finite</em> but not that it goes to zero. There is a solid chance that the Cauchy-Schwartz inequality <span class="math display">\[
\frac{Mr_{S-M+1:S}^{1/2}}{S(1-R(r_{S-M+1:S}))} \mathbb{E}(r_{S-M+1:S}^{1/2}H 1_{R \geq r_{S-M+1:S}}) \leq\frac{Mr_{S-M+1:S}^{1/2}}{S(1-R(r_{S-M+1:S}))}R(r_{S-M+1:S})\|h\|_{L^2(p)}
\]</span> would work. But truly that is just bloody messy<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a>.</p>
<p>So let’s do it the easy way, shall we. Fundamentally, we will use <span class="math display">\[
\mathbb{V}\left(I_\text{WIS}^S\right) \leq \mathbb{E}\left([I_\text{WIS}^S]^2\right).
\]</span> Noting that we can write <span class="math inline">\(I_\text{WIS}^S\)</span> compactly as <span class="math display">\[
I_\text{WIS}^S = \frac{1}{S}\sum_{s=1}^S h(\theta_s)\min\{r(\theta_s), r_{S-M+1:S}\}.
\]</span> Hence, <span class="math display">\[\begin{align*}
\mathbb{E}\left([I_\text{WIS}^S]^2\right) &amp;= \mathbb{E}_{T\sim r_{S-M+1:S}}\left[\mathbb{E}\left([I_\text{WIS}^S]^2 \mid r_{S-M+1:S} = T\right)\right]\\
&amp;=\frac{1}{S^2}\mathbb{E}_{T\sim r_{S-M+1:S}}\left[\mathbb{E}\left(H^2 \min\{R^2,T^2\} \mid r_{S-M+1:S} = T\right)\right]\\
&amp;\leq\frac{1}{S^2}\mathbb{E}_{T\sim r_{S-M+1:S}}\left[\mathbb{E}\left(RTH^2 \mid r_{S-M+1:S} = T\right)\right] \\
&amp;\leq\frac{1}{S^2}\mathbb{E}_{T\sim r_{S-M+1:S}}\left[T\|h\|_{L^2(p)}^2\right]
\end{align*}\]</span></p>
<p>This goes to zero as long as <span class="math inline">\(\mathbb{E}(r_{S-M+1:S}) = o(S^2)\)</span>.</p>
<p><a href="https://projecteuclid.org/proceedings/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fifth-Berkeley-Symposium-on-Mathematical-Statistics-and/Chapter/Some-contributions-to-the-theory-of-order-statistics/bsmsp/1200513012">Bickel (1967)</a> shows that, noting that <span class="math inline">\(\mathbb{E}(R) &lt; \infty\)</span>, <span class="math display">\[
\mathbb{E}(r_{S-M+1:S}) \leq C (S-M+1)\frac{\Gamma(S+1)\Gamma(S-M+1-1)\Gamma(M)}{\Gamma(S-M+1+1)\Gamma(M+1)\Gamma(S-1)} = \frac{S}{M}(1 + o(1)),
\]</span> and so the variance is bounded.</p>
<p>The previous argument shows that the variance is <span class="math inline">\(\mathcal{O}(M^{-1}S^{-1})\)</span>. We can refine that if we assume the von Mises condition hold. In that case we know that <span class="math inline">\(R(r) = 1- cr^{-1/k} + o(1)\)</span> as <span class="math inline">\(r\rightarrow \infty\)</span> and therefore <span class="math display">\[\begin{align*}
R\left(R^{-1}\left(1-\frac{M}{S}\right)\right) &amp;= 1-\frac{M}{S+1}\\
1 - cR^{-1}\left(1-\frac{M}{S+1}\right)^{-1/k}(1+o(1)) &amp;= 1- \frac{M}{S+1} \\
R^{-1}\left(1-\frac{M}{S+1}\right) &amp;= c^{-k}\left(\frac{M}{S+1}\right)^{-k}(1 + o(1)).
\end{align*}\]</span> Bickel (1967) shows that <span class="math inline">\(\mathbb{E}(r_{k:S}) = R^{-1}(1-M/(S+1)) + o(1)\)</span> so combining this with the previous result gives a variance of <span class="math inline">\(\mathcal{O}((M/S)^{k-2})\)</span>. If we take <span class="math inline">\(M =\mathcal{O}(S^{1/2})\)</span>, this gives <span class="math inline">\(\mathcal{S}^{k/2-1}\)</span>, which is smaller than the previous bound for <span class="math inline">\(k&lt;1\)</span>. It’s worth noting that Hence the variance goes to zero.</p>
<p>The argument that we used here is a modification of the argument in the TIS paper. This lead to a great deal of panic: did I just make my life extremely difficult? Could I have modified the TIS proof to show the bias goes to zero? To be honest, someone might be able to, but I can’t.</p>
<p>So anyway, we’ve proved the following theorem.</p>
<div id="thm-thm2" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 </strong></span>Let <span class="math inline">\(\theta_s\)</span>, <span class="math inline">\(s = 1,\ldots, S\)</span> be an iid sample from <span class="math inline">\(G\)</span> and let <span class="math inline">\(r_s = r(\theta_s) \sim R\)</span>. Assume that</p>
<ol type="1">
<li><p><span class="math inline">\(R\)</span> is absolutely continuous</p></li>
<li><p><span class="math inline">\(M \rightarrow \infty\)</span> and <span class="math inline">\(M^{-1}S \rightarrow 0\)</span></p></li>
<li><p><span class="math inline">\(h \in L^2(p)\)</span>.</p></li>
</ol>
<p>The variance in Winsorized importance sampling is at most <span class="math inline">\(\mathcal{O}(M^{-1}S)\)</span>.</p>
</div>
</section>
</section>
<section id="pareto-smoothed-importance-sampling" class="level2">
<h2 class="anchored" data-anchor-id="pareto-smoothed-importance-sampling">Pareto-smoothed importance sampling</h2>
<p>Pareto-smoothed importance sampling (or PSIS) takes the observation that the tails are approximately Pareto distributed to add some bias correction to the mix. Essentially, it works by noting that approximating <span class="math display">\[
(1-R(r_{S-M+1:S}))\mathbb{E}(HR \mid R&gt;r_{S-M+1:S}) \approx \frac{1}{S}\sum_{m=1}^M w_m h_{S-M+m:S},
\]</span> where <span class="math inline">\(w_m\)</span> is the median<a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a> <span class="math inline">\(m\)</span>th order statistic in an iid sample of <span class="math inline">\(M\)</span> Generalised Pareto random variables with tail parameters fitted to the distribution.</p>
<p>This is a … funky … quadrature rule. To see that, we can write <span class="math display">\[
\mathbb{E}(HR \mid R&gt;T) = \mathbb{E}(R \mathbb{E}(H \mid R)).
\]</span> If we approximate the distribution of <span class="math inline">\(R &gt; T\)</span> by <span class="math display">\[
\tilde{R}_\text{PSIS}(r) = \frac{1}{M}\sum_{m=1}^M 1( w_m&lt;r)
\]</span> and approximate the conditional probability by <span class="math display">\[
\Pr(H &lt; h\mid R = w_m) \approx 1(h_{S-M+m:S}&lt; h).
\]</span></p>
<p>Empirically, this is a very good choice (with the mild caveat that you need to truncate the largest expected order statistic by the observed maximum in order to avoid some variability issues). I would love to have a good analysis of why that is so, but honest I do not.</p>
<p>But, to the issue of this blog post the convergence and vanishing variance still holds. To see this, we note that <span class="math display">\[
w_m = r_{S-M+1}  + k^{-1}\sigma\left[\left(1-\frac{j-1/2}{M}\right)^{-k} -1\right].
\]</span> So we are just re-weighting our tail <span class="math inline">\(H\)</span> samples by <span class="math display">\[
1 + \frac{\sigma}{kr_{S-M+1:S}}\left[\left(1-\frac{j-1/2}{M}\right)^{-k} -1\right].
\]</span></p>
<p>Recalling that when <span class="math inline">\(R(r) = 1- cr^{-1/k}(1+ o(1))\)</span>, we had <span class="math inline">\(\sigma = \mathcal{O}(r_{S-M+1:S})\)</span>, this term is at most <span class="math inline">\(\mathcal{O}(1 + M^{-k})\)</span>. This will not trouble either of our convergence proofs.</p>
<p>This leads to the following modification of our previous results.</p>
<div id="thm-thm3" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3 </strong></span>Let <span class="math inline">\(\theta_s\)</span>, <span class="math inline">\(s = 1,\ldots, S\)</span> be an iid sample from <span class="math inline">\(G\)</span> and let <span class="math inline">\(r_s = r(\theta_s) \sim R\)</span>. Assume that</p>
<ol type="1">
<li><p><span class="math inline">\(R\)</span> is absolutely continuous.</p></li>
<li><p><span class="math inline">\(M = \mathcal{O}(S^{1/2})\)</span></p></li>
<li><p><span class="math inline">\(h \in L^2(p)\)</span></p></li>
<li><p><span class="math inline">\(k\)</span> and <span class="math inline">\(\sigma\)</span> are known with <span class="math inline">\(\sigma = \mathcal{O}(r_{S-M+1:S})\)</span>.</p></li>
</ol>
<p>Pareto smoothed importance sampling converges in <span class="math inline">\(L^1\)</span> and its variance goes to zero and it is consistent and asymptotically unbiased.</p>
</div>
<div id="cor-cor2" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 2 </strong></span>Assume further that</p>
<ol type="1">
<li><p>R satisfies the von Mises condition<a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a> <span class="math display">\[
\frac{rR'(r)}{1-R(r)} = \frac{1}{k}(1 +\mathcal{O}(r^{-1})).
\]</span></p></li>
<li><p><span class="math inline">\(h\)</span> is bounded<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a>.</p></li>
</ol>
<p>Then the L^1 convergence occurs at a rate of of, at most, <span class="math inline">\(\mathcal{O}(S^{-1/2})\)</span>. Furthermore, the variance of the PSIS estimator goes to zero at least as fast as <span class="math inline">\(\mathcal{O}(S^{k/2-1})\)</span>.</p>
</div>
<p>Hence, under these additional conditions PSIS is<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a> <span class="math inline">\(\sqrt{n}\)</span>-consistent.</p>
</section>
<section id="final-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="final-thoughts">Final thoughts</h2>
<p>So that’s what truncation and winsorization does to importance sampling estimates. I haven’t touched on the fairly important topic of asymptotic normality. Essentially, <a href="https://www.sciencedirect.com/science/article/pii/0304414988900312">Griffin (1988)</a>, in a fairly complex<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a> paper that suggests that if you winsorize the product <span class="math inline">\((h(\theta_s)r(\theta_s))\)</span> <em>and</em> winsorize it at both ends, the von Mises condition<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a> imply that the WIS estimator is asymptotically normal.</p>
<p>Why is this important, well the same proof shows that doubly winsorized importance sampling (dWIS) applied to the vector valued function <span class="math inline">\(\tilde h(\theta) = (h(\theta),1)\)</span> will also be asymptotically normal, which implies, via the delta method, that the <em>self normalized</em> dWIS estimator <span class="math display">\[
I^S_\text{SN-IS} = \frac{\sum_{s=1}^S\max\{\min\{h(\theta_i) r(\theta_i),T_{S-M+1:S}\}, T_{M:S}\}}{\sum_{s=1}^S\max\{\min\{r(\theta_i),T_{S-M+1:S}\},T_{M:S}\}}
\]</span> is consistent, where <span class="math inline">\(T_{m:S}\)</span> is the <span class="math inline">\(m\)</span>th order statistic of <span class="math inline">\(\max\{h(\theta_s)r(\theta_s), r(\theta_s)\}\)</span>.</p>
<p>It is very very likely that this can be shown (perhaps under some assumptions) for something closer to the version of PSIS we use in practice. But that is an open question.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>proportional to<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>because <span class="math inline">\(p(\theta_s)\)</span> is very small<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>because <span class="math inline">\(p(\theta_s)\)</span> is a reasonable size, but <span class="math inline">\(g(\theta_s)\)</span> is tiny.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>I have surreptitiously dropped the <span class="math inline">\(h\)</span> subscript because I am gay and sneaky.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>That it’s parameterised by <span class="math inline">\(1/k\)</span> is an artefact of history.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>We need <span class="math inline">\(\mathbb{E}(R)\)</span> to be finite, so we need <span class="math inline">\(k&lt;1\)</span>.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>very fucking complex<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>I have used that old trick of using the same letter for the CDF as the random variable when I have a lot of random variables. <a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>aka the tail index<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>This is a relevant case. But if you think a little bit about it, our problem happens when <span class="math inline">\(r(\theta)\)</span> grows <em>much</em> faster than <span class="math inline">\(h(\theta)\)</span>. For example if <span class="math inline">\(P = \operatorname{Exp}(1)\)</span> and <span class="math inline">\(G = \operatorname{Exp}(1/\lambda)\)</span> for <span class="math inline">\(\lambda&gt;1\)</span>, then <span class="math inline">\(k = 1-1/\lambda\)</span>, <span class="math inline">\(r(\theta) = \exp((\lambda-1)\theta)\)</span> and if <span class="math inline">\(|h(\theta)| &lt; |\theta|^\alpha\)</span>, then <span class="math inline">\(|h(\theta)| \leq C \log(r)^\alpha\)</span>, which is a slowly growing function.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Because the truncation depends on <span class="math inline">\(S\)</span>, moving from the <span class="math inline">\(S\)</span>th partial sum to the <span class="math inline">\(S+1\)</span>th partial sum changes the distribution of <span class="math inline">\(z_ih_ir_i\)</span>. This is exactly why the dead Russians gifted us with triangular arrays.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>Also practical unbounded <span class="math inline">\(h\)</span>, but it’s just easier for bounded <span class="math inline">\(h\)</span><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>Shut up. I know. Don’t care.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>or, hell, even in a book<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>Straight up, though, I spent 2 days dicking around with tail bounds on sums of Bernoulli random variables for some bloody reason before I just looked at the damn formula.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>Ok. I checked. And yeah. Same technique as below using Jensen in its <span class="math inline">\(\mathbb{E}(|X-\mathbb{E}(X)|)^2 \leq \mathbb{V}(X)\)</span>. If you put that together you get something that goes to zero like <span class="math inline">\(M^{1/2}S^{-1}\)</span>, which is <span class="math inline">\(\mathcal{O}(S^{-3/4})\)</span> for our usual choice of <span class="math inline">\(M\)</span>. Which confirms the suspicion that the first term in the bias goes to zero <em>much</em> faster than the second (remembering, of course, that Jensen’s inequality is notoriously loose!).<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>It’s Pride month<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18"><p>The result holds exactly if <span class="math inline">\(\mathbb{E}(H \mid R=r) = \mathcal{O}(\log^k(r))\)</span> and with a <span class="math inline">\(k\)</span> turning up somewhere if it’s <span class="math inline">\(o(r^{1/k - 1})\)</span>.<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19"><p><span class="math inline">\(1-R(r) \sim c r^{(-1/k)}\mathcal{L(r)}\)</span> for a slowly varying function (eg a power of a logarithm) <span class="math inline">\(\mathcal{L}(r)\)</span>.<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20"><p>A property that implies this is that <span class="math inline">\(1-R(r)\)</span> is differentiable and <em>convex at infinity</em>, which is to say that there is some finite <span class="math inline">\(r_0\)</span> such that <span class="math inline">\(R'(r)\)</span> exists for all <span class="math inline">\(r \geq r_0\)</span> and <span class="math inline">\(1-R(r)\)</span> is a monotone function on <span class="math inline">\([r_0, \infty)\)</span>.<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21"><p>There’s a condition here that <span class="math inline">\(S\)</span> has to be large enough, but it’s enough if <span class="math inline">\((S-M+1) &gt; 2\)</span>.<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22"><p>The first <span class="math inline">\(k\)</span> in the equation below is missing in the paper. If you miss this, you suddenly get the expected value converging to zero, which would be <em>very</em> surprising. Always sense-check the proofs, people. Even if a famous person did it in the 60s.<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23"><p>We need to take <span class="math inline">\(M = \mathcal{O}(S^{1/2})\)</span> to be able to estimate the tail index <span class="math inline">\(k\)</span> from a sample, which gives an upper bound by a constant.<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24"><p>Note that if <span class="math inline">\(U \sim \text{Unif}(0,1)\)</span>, then <span class="math inline">\(R^{-1}(U) \sim R\)</span>. Because this is monotone, it doesn’t change ordering of the sample<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25"><p>This is, incidentally, how Bickel got the upper bound on the moments. He combined this with an upper bound on the quantile function.<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn26"><p>Save the cheerleader, save the world. Except it’s one minus a beta is still beta but with the parameters reversed.<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn27"><p>As long as <span class="math inline">\(M = o(S)\)</span><a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn28"><p>The rate here is probably not optimal, but it will guarantee that the error in the Pareto approximation doesn’t swamp the other terms.<a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn29"><p>Or <span class="math inline">\(\mathbb{E}(h(\theta) \mid r(\theta) = r)\)</span> doesn’t grow to quickly, with some modification of the rates in the unlikely case that it grows polynomially.<a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn30"><p>almost, there’s an epsilon gap but I don’t give a shit<a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn31"><p>And girl do not get me started on messy. I ended up going down a route where I used the [inequality]((https://www.sciencedirect.com/science/article/pii/0167715288900077) <span class="math display">\[
\mathbb{V}(g(U)) \leq \mathbb{E}(U)\int_0^1\left[F_U(u) - \frac{\mathbb{E}(U1_{U\leq u})}{\mathbb{E}(U)}\right][g'(u)]^2\,du
\]</span> which holds for any <span class="math inline">\(U\)</span> supported on <span class="math inline">\([0,1]\)</span> with differentiable density. And let me tell you. If you dick around with enough beta distributions you can get something. Is it what you want? Fucking no. It is <em>a lot</em> of work, including having to differentiate the conditional expectation, and it gives you sweet bugger all.<a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn32"><p>Or, the expected within <span class="math inline">\(o(S^{-1/2})\)</span><a href="#fnref32" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn33"><p>The rate here is probably not optimal, but it will guarantee that the error in the Pareto approximation doesn’t swamp the other terms.<a href="#fnref33" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn34"><p>Or <span class="math inline">\(\mathbb{E}(h(\theta) \mid r(\theta) = r)\)</span> doesn’t grow to quickly, with some modification of the rates in the unlikely case that it grows polynomially.<a href="#fnref34" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn35"><p>almost, there’s an epsilon gap but I don’t give a shit<a href="#fnref35" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn36"><p>I mean, the tools are elementary. It’s just a lot of detailed estimates and Berry-Esseen as far as the eye can see.<a href="#fnref36" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn37"><p>and more general things<a href="#fnref37" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">https://creativecommons.org/licenses/by-nc/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{simpson2022,
  author = {Dan Simpson},
  editor = {},
  title = {Tail Stabilization of Importance Sampling Etimators: {A} Bit
    of Theory},
  date = {2022-06-15},
  url = {https://dansblog.netlify.app/2022-06-03-that-psis-proof},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-simpson2022" class="csl-entry quarto-appendix-citeas" role="doc-biblioentry">
Dan Simpson. 2022. <span>“Tail Stabilization of Importance Sampling
Etimators: A Bit of Theory.”</span> June 15, 2022. <a href="https://dansblog.netlify.app/2022-06-03-that-psis-proof">https://dansblog.netlify.app/2022-06-03-that-psis-proof</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>